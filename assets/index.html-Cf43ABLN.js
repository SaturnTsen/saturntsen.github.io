import{_ as n,c as e,a as t,d as s,o as i}from"./app-BPmJL-vo.js";const l="/images/um-cv/12-1.png",p="/images/um-cv/12-2.png",m="/images/um-cv/12-3.png",r="/images/um-cv/12-4.png",o="/images/um-cv/12-5.png",c="/images/um-cv/12-6.png",h="/images/um-cv/12-7.png",g="/images/um-cv/12-8.png",d="/images/um-cv/12-9.png",u="/images/um-cv/12-10.png",v="/images/um-cv/12-11.png",y="/images/um-cv/12-12.png",b="/images/um-cv/12-13.png",f="/images/um-cv/12-14.png",w="/images/um-cv/12-15.png",x="/images/um-cv/12-16.png",_="/images/um-cv/12-17.png",z="/images/um-cv/12-18.png",k="/images/um-cv/12-19.png",N="/images/um-cv/12-20.png",S="/images/um-cv/12-21.png",M="/images/um-cv/12-22.png",A="/images/um-cv/12-23.png",q="/images/um-cv/12-24.png",T="/images/um-cv/12-25.png",R="/images/um-cv/13-1.png",L="/images/um-cv/13-2.png",C="/images/um-cv/13-3.png",G="/images/um-cv/13-4.png",P="/images/um-cv/13-5.png",I="/images/um-cv/13-6.png",U="/images/um-cv/13-7.png",W="/images/um-cv/13-8.png",E="/images/um-cv/13-9.png",V="/images/um-cv/13-10.png",O="/images/um-cv/13-11.png",B="/images/um-cv/13-12.png",F="/images/um-cv/13-13.png",H="/images/um-cv/13-14.png",Y="/images/um-cv/13-15.png",D="/images/um-cv/13-16.png",j="/images/um-cv/13-17.png",Q="/images/um-cv/13-18.png",Z="/images/um-cv/13-19.png",X="/images/um-cv/13-20.png",J="/images/um-cv/13-21.png",K={};function $(ss,a){return i(),e("div",null,[...a[0]||(a[0]=[t('<p>Part 1: RNNs, Vanilla Rnns, LSTMs, GRU, Gradient explosion, Gradient vanishing, Architecture Search, Empirical Understanding of RNNs.</p><p>Part 2: Seq2Seq, Attention Mechanism, Self-Attention, Multi-head Attention, Transformers, Scaling up Transformers.</p><p>@Credits: <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/" target="_blank" rel="noopener noreferrer">EECS 498.007</a> | Video Lecture: <a href="https://www.youtube.com/watch?v=dJYGatp4SvA&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r" target="_blank" rel="noopener noreferrer">UM-CV</a></p><p>Personal work for the assignments of the course: <a href="https://github.com/SaturnTsen/EECS-498-007/" target="_blank" rel="noopener noreferrer">github repo</a>.</p><p><strong>Notice on Usage and Attribution</strong></p><p>These are personal class notes based on the University of Michigan EECS 498.008 / 598.008 course. They are intended solely for personal learning and academic discussion, with no commercial use.</p><p>For detailed information, please refer to the <strong><a href="#notice-on-usage-and-attribution">complete notice at the end of this document</a></strong></p><h2 id="intro" tabindex="-1"><a class="header-anchor" href="#intro"><span>Intro</span></a></h2><h3 id="process-sequences" tabindex="-1"><a class="header-anchor" href="#process-sequences"><span>Process Sequences</span></a></h3><ul><li>one to one: standard feed-forward network</li><li>one to many: image captioning</li><li>many to one: sentiment analysis, image classification</li><li>many to many: machine translation/per-frame video classification</li></ul><h3 id="sequential-processing-of-non-sequential-data" tabindex="-1"><a class="header-anchor" href="#sequential-processing-of-non-sequential-data"><span>Sequential Processing of Non-Sequential Data</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+l+'" width="70%" alt="Sequential Processing of Non-Sequential Data"><br> Sequential Processing of Non-Sequential Data: Classification</div><div style="text-align:center;margin-bottom:1em;"><img src="'+p+'" width="70%" alt="Sequential Processing of Non-Sequential Data"><br> Sequential Processing of Non-Sequential Data: Generation</div><h2 id="recurrent-neural-networks" tabindex="-1"><a class="header-anchor" href="#recurrent-neural-networks"><span>Recurrent Neural Networks</span></a></h2><h3 id="architecture" tabindex="-1"><a class="header-anchor" href="#architecture"><span>Architecture</span></a></h3><p>Key idea: RNNs maintain a hidden state that is updated at each time step.</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><msub><mi>f</mi><mi>W</mi></msub><mo stretchy="false">(</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t = f_W(h_{t-1}, x_t) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p>where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">h_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is the hidden state at time <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is the input at time <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>, and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>W</mi></msub></mrow><annotation encoding="application/x-tex">f_W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is a function parameterized by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>.</p><p>Vanilla Recurrent Neural Networks:</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>W</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>W</mi><mrow><mi>x</mi><mi>h</mi></mrow></msub><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">hh</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>',20),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mtable",{rowspacing:"0.25em",columnalign:"right left",columnspacing:"0em"},[s("mtr",null,[s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"true"},[s("msub",null,[s("mi",null,"h"),s("mi",null,"t")])])]),s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"true"},[s("mrow",null,[s("mrow"),s("mo",null,"="),s("mi",null,"tanh"),s("mo",null,"⁡"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"W"),s("mrow",null,[s("mi",null,"h"),s("mi",null,"h")])]),s("msub",null,[s("mi",null,"h"),s("mrow",null,[s("mi",null,"t"),s("mo",null,"−"),s("mn",null,"1")])]),s("mo",null,"+"),s("msub",null,[s("mi",null,"W"),s("mrow",null,[s("mi",null,"x"),s("mi",null,"h")])]),s("msub",null,[s("mi",null,"x"),s("mi",null,"t")]),s("mo",{stretchy:"false"},")")])])])]),s("mtr",null,[s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"true"},[s("msub",null,[s("mi",null,"y"),s("mi",null,"t")])])]),s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"true"},[s("mrow",null,[s("mrow"),s("mo",null,"="),s("msub",null,[s("mi",null,"W"),s("mrow",null,[s("mi",null,"h"),s("mi",null,"y")])]),s("msub",null,[s("mi",null,"h"),s("mi",null,"t")])])])])])]),s("annotation",{encoding:"application/x-tex"},"\\begin{aligned} h_t &= \\tanh(W_{hh}h_{t-1} + W_{xh}x_t) \\\\ y_t &= W_{hy}h_t \\end{aligned} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"3em","vertical-align":"-1.25em"}}),s("span",{class:"mord"},[s("span",{class:"mtable"},[s("span",{class:"col-align-r"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.75em"}},[s("span",{style:{top:"-3.91em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"h"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])]),s("span",{style:{top:"-2.41em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"y"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.25em"}},[s("span")])])])]),s("span",{class:"col-align-l"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.75em"}},[s("span",{style:{top:"-3.91em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"}),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mop"},"tanh"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"hh")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"h"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3011em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"t"),s("span",{class:"mbin mtight"},"−"),s("span",{class:"mord mtight"},"1")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2083em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"x"),s("span",{class:"mord mathnormal mtight"},"h")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")")])]),s("span",{style:{top:"-2.41em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"}),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"W"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"h"),s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"y")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])]),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"h"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.25em"}},[s("span")])])])])])])])])])])],-1),t('<h3 id="computational-graph" tabindex="-1"><a class="header-anchor" href="#computational-graph"><span>Computational Graph</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+m+'" width="70%" alt="Computational Graph of RNN"><br> Computational Graph of RNN</div><p>Many to many:</p><div style="text-align:center;margin-bottom:1em;"><img src="'+r+'" width="70%" alt="Computational Graph of RNN"><br> Computational Graph of RNN: Many to Many</div><p>Many to one: Encode input sequence in a single vector. See <a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="noopener noreferrer">Sequence to Sequence Learning with Neural Networks</a>.</p><p>One to many: Produce output sequence from single input vector.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+o+'" width="70%" alt="Seq2Seq"><br> Seq2Seq</div><p>Example: Language Modeling</p><div style="text-align:center;margin-bottom:1em;"><img src="'+c+'" width="70%" alt="Language Modeling"><br> Language Modeling</div><p>Given &quot;h&quot;, predict &quot;e&quot;, given &quot;hell&quot;, predict &quot;o&quot;.</p><p>So far: encode inputs as one-hot-vector -&gt; Embedding layer</p><div style="text-align:center;margin-bottom:1em;"><img src="'+h+'" width="70%" alt="Embedding layer"><br> Embedding layer</div><h3 id="backpropagation-through-time" tabindex="-1"><a class="header-anchor" href="#backpropagation-through-time"><span>Backpropagation Through Time</span></a></h3><ul><li>Problem: Takes a lot of memory for long sequences.</li><li>Unroll the RNN for a fixed number of time steps.</li><li>Solution: Truncated chunks of the sequence.</li></ul><div style="text-align:center;margin-bottom:1em;"><img src="'+g+'" width="70%" alt="Backpropagation Through Time"><br> Backpropagation Through Time</div><p>Minimal implementation: <a href="https://gist.github.com/karpathy/d4dee566867f8291f086" target="_blank" rel="noopener noreferrer">min-char-rnn.py</a></p><h3 id="training-rnns" tabindex="-1"><a class="header-anchor" href="#training-rnns"><span>Training RNNs</span></a></h3><p>Shakespeare&#39;s Sonnet, Algebraic Geometry Textbook LaTeX code, Generated C Code</p><div style="text-align:center;margin-bottom:1em;"><img src="'+d+'" width="70%" alt="Training RNNs"><br> Training RNNs</div><h3 id="searching-for-interpretable-hidden-units" tabindex="-1"><a class="header-anchor" href="#searching-for-interpretable-hidden-units"><span>Searching for Interpretable Hidden Units</span></a></h3><p>Visualizing and Understanding Recurrent Networks: <a href="https://arxiv.org/abs/1506.02078" target="_blank" rel="noopener noreferrer">arXiv:1506.02078</a></p><div style="text-align:center;margin-bottom:1em;"><img src="'+u+'" width="70%" alt="Searching for Interpretable Hidden Units"><br> Quote deletion cell</div><div style="text-align:center;margin-bottom:1em;"><img src="'+v+'" width="70%" alt="Searching for Interpretable Hidden Units"><br> line length tracking cell</div><div style="text-align:center;margin-bottom:1em;"><img src="'+y+'" width="70%" alt="Searching for Interpretable Hidden Units"><br> If statement cell</div><p>Example: Image captioning</p><div style="text-align:center;margin-bottom:1em;"><img src="'+b+'" width="70%" alt="Searching for Interpretable Hidden Units"><br> Image captioning</div><p>Transfer learning from CNN, then add RNN.</p><p>Results: <a href="https://arxiv.org/abs/1411.4555" target="_blank" rel="noopener noreferrer">arXiv:1411.4555</a></p><div style="text-align:center;margin-bottom:1em;"><img src="'+f+'" width="70%" alt="Searching for Interpretable Hidden Units"><br> Image captioning results</div><div style="text-align:center;margin-bottom:1em;"><img src="'+w+'" width="70%" alt="Searching for Interpretable Hidden Units"><br> Image captioning results</div><p>Failure Cases:</p><div style="text-align:center;margin-bottom:1em;"><img src="'+x+'" width="70%" alt="Searching for Interpretable Hidden Units"><br> Failure Cases</div><h3 id="gradient-flow" tabindex="-1"><a class="header-anchor" href="#gradient-flow"><span>Gradient Flow</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+_+'" width="70%" alt="Gradient Flow"><br> Gradient Flow</div><p>Computing gradient of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">h_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> involves many factors of W (and repeated tanh)</p><p>Gradient Clipping: Scale gradients if they get too large.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+z+'" width="70%" alt="Gradient Clipping"><br> Gradient Clipping</div><p>Vanishing Gradient: If the gradient is too small, the weights won&#39;t change -&gt; Change the architecture.</p><h3 id="long-short-term-memory-lstm" tabindex="-1"><a class="header-anchor" href="#long-short-term-memory-lstm"><span>Long Short Term Memory (LSTM)</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+k+'" width="70%" alt="LSTM"><br> LSTM (1997)</div><div style="text-align:center;margin-bottom:1em;"><img src="'+N+'" width="70%" alt="LSTM"><br> LSTM</div><div style="text-align:center;margin-bottom:1em;"><img src="'+S+'" width="70%" alt="LSTM"><br> LSTM</div><p>Uninterrupted gradient flow!</p><div style="text-align:center;margin-bottom:1em;"><img src="'+M+'" width="70%" alt="LSTM"><br> LSTM</div><h3 id="multi-layer-rnns" tabindex="-1"><a class="header-anchor" href="#multi-layer-rnns"><span>Multi-layer RNNs</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+A+'" width="70%" alt="Two-layer RNN"><br> Two-layer RNN</div><h3 id="other-rnn-variants" tabindex="-1"><a class="header-anchor" href="#other-rnn-variants"><span>Other RNN Variants</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+q+'" width="70%" alt="Other RNN Variants"><br>Search for RNN architectures empirically.</div><div style="text-align:center;margin-bottom:1em;"><img src="'+T+'" width="70%" alt="Other RNN Variants"><br>Neural Architecture Search.</div><h2 id="attention-mechanism" tabindex="-1"><a class="header-anchor" href="#attention-mechanism"><span>Attention Mechanism</span></a></h2><p>Problem of Attention: Long sequences are hard to process.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+R+'" width="70%" alt="Attention Mechanism"><br> Attention Mechanism</div><h3 id="seq-to-seq-with-rnns-and-attention" tabindex="-1"><a class="header-anchor" href="#seq-to-seq-with-rnns-and-attention"><span>Seq to Seq with RNNs and Attention</span></a></h3><p>Compute (scalar) alignment scores.</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>e</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><mo>=</mo><msub><mi>f</mi><mrow><mi>a</mi><mi>t</mi><mi>t</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">e_{t,i} = f_{att}(s_{t-1}, h_i) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">tt</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>a</mi><mi>t</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">f_{att}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">tt</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is an MLP</p><p>Normalize alignment scores to get attention weights <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>&lt;</mo><msub><mi>a</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0 &lt; a_{t,i} &lt; 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6835em;vertical-align:-0.0391em;"></span><span class="mord">0</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8252em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mi>i</mi></msub><msub><mi>a</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\\sum_i a_{t,i} = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></p><div style="text-align:center;margin-bottom:1em;"><img src="'+L+'" width="70%" alt="Seq to Seq with RNNs and Attention"><br> RNN</div><p>Compute context vector as weighted sum of encoder hidden states.</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">c_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> = <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mi>i</mi></msub><msub><mi>a</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\\sum_i a_{t,i}h_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p><p>Use the context vector as input to the decoder:</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><msub><mi>g</mi><mi>U</mi></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>c</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s_t = g_{U}(s_{t-1}, y_{t-1}, c_t) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><div style="text-align:center;margin-bottom:1em;"><img src="'+C+'" width="70%" alt="Seq to Seq with RNNs and Attention"><br> Seq to Seq with RNNs and Attention</div><p>We do not need to tell the model to pay attention to which part of the input.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+G+'" width="70%" alt="Seq to Seq with RNNs and Attention"><br> Seq to Seq with RNNs and Attention</div><p>Rather than trying to stuff all the information in a single vector, we give the model the ability to attend to different parts of the input.</p><p>Example: Translation Takes</p><div style="text-align:center;margin-bottom:1em;"><img src="'+P+'" width="70%" alt="Attention Matrix"><br> Attention Matrix</div><h3 id="image-captioning-with-rnns-and-attention" tabindex="-1"><a class="header-anchor" href="#image-captioning-with-rnns-and-attention"><span>Image Captioning with RNNs and Attention</span></a></h3><p>CNN -&gt; Attention to get Alignment Scores -&gt; RNN</p><div style="text-align:center;margin-bottom:1em;"><img src="'+I+'" width="70%" alt="Image Captioning with RNNs and Attention"><br></div><div style="text-align:center;margin-bottom:1em;"><img src="'+U+'" width="70%" alt="Image Captioning with RNNs and Attention"><br> Image Captioning with RNNs and Attention</div><p><a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="noopener noreferrer">Neural Image Caption Generation with Visual Attention</a></p><div style="text-align:center;margin-bottom:1em;"><img src="'+W+'" width="70%" alt="Image Captioning with RNNs and Attention"><br> Area to which the attention is attributed</div><h4 id="biological-inspiration" tabindex="-1"><a class="header-anchor" href="#biological-inspiration"><span>Biological Inspiration</span></a></h4><p>Our retina has a fovea, which is a high-resolution area in the center of our vision. Our eyes move around constantly to focus on different parts of the image so we don&#39;t notice.</p><p>Retina的中文是视网膜，Fovea是视网膜的中心区域，视网膜的中心区域是视觉最清晰的地 方，也是视觉最敏锐的地方。</p><h4 id="x-attend-and-y" tabindex="-1"><a class="header-anchor" href="#x-attend-and-y"><span>X, attend, and Y</span></a></h4><ul><li><a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="noopener noreferrer">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></li><li><a href="https://arxiv.org/abs/1806.01214" target="_blank" rel="noopener noreferrer">Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering</a></li><li><a href="https://arxiv.org/abs/1508.01211" target="_blank" rel="noopener noreferrer">Listen, Attend and Spell</a></li><li><a href="https://arxiv.org/abs/1606.02245" target="_blank" rel="noopener noreferrer">Listen, Attend and Walk</a></li><li><a href="https://arxiv.org/abs/1704.03054" target="_blank" rel="noopener noreferrer">Show, Attend, and Interact</a></li><li><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">Show, Attend and Read</a></li></ul><h3 id="general-purpose-attention-layer" tabindex="-1"><a class="header-anchor" href="#general-purpose-attention-layer"><span>General-Purpose Attention Layer</span></a></h3><p>Inputs: Query vector, Input vectors, and Similarity function. Computation: Similarities, Attention weights, Output vector.</p><h4 id="_1st-generalization" tabindex="-1"><a class="header-anchor" href="#_1st-generalization"><span>1st generalization</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="'+E+'" width="70%" alt="General-Purpose Attention Layer"><br> General-Purpose Attention Layer</div><p>Use scaled dot product for similarity</p><h4 id="_2nd-generalization-multiple-query-vectors" tabindex="-1"><a class="header-anchor" href="#_2nd-generalization-multiple-query-vectors"><span>2nd generalization: Multiple query vectors</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="'+V+'" width="70%" alt="General-Purpose Attention Layer"><br> General-Purpose Attention Layer</div><h4 id="_3rd-generalization-query-key-value-attention" tabindex="-1"><a class="header-anchor" href="#_3rd-generalization-query-key-value-attention"><span>3rd generalization: Query-Key-Value Attention</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="'+O+'" width="70%" alt="General-Purpose Attention Layer"><br> General-Purpose Attention Layer</div><h3 id="self-attention-layer" tabindex="-1"><a class="header-anchor" href="#self-attention-layer"><span>Self-Attention Layer</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+B+'" width="70%" alt="Self-Attention Layer"><br> Self-Attention Layer</div><p>Problem: Self-attention is permutation invariant. It does not care about order all.</p><p>Solution: Positional encoding. We append a vector indicating the position of the word.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+F+'" width="70%" alt="Self-Attention Layer"><br> Self-Attention Layer</div><h3 id="masked-self-attention" tabindex="-1"><a class="header-anchor" href="#masked-self-attention"><span>Masked Self-Attention</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+H+'" width="70%" alt="Masked Self-Attention"><br> Masked Self-Attention</div><h3 id="multi-head-self-attention" tabindex="-1"><a class="header-anchor" href="#multi-head-self-attention"><span>Multi-head Self-Attention</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+Y+'" width="70%" alt="Multi-head Self-Attention"><br> Multi-head Self-Attention</div><h3 id="example-cnn-with-self-attention" tabindex="-1"><a class="header-anchor" href="#example-cnn-with-self-attention"><span>Example: CNN with Self-Attention</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+D+'" width="70%" alt="CNN with Self-Attention"><br> CNN with Self-Attention</div><h2 id="three-ways-of-processing-sequences" tabindex="-1"><a class="header-anchor" href="#three-ways-of-processing-sequences"><span>Three ways of Processing Sequences</span></a></h2><ol><li>RNN: works on ordered sequences, is good at long sequences: After one RNN layer, h_t sees the whole sequence. But it is not parallelizable.</li><li>1D Convolution: Works on Multidimensional Grids. It is not good at long sequences. It is highly parallelizable</li><li>Self-Attention: Works on unordered sequences. It is good at long sequences. After one self-attention layer, each word sees the whole sequence. It is highly parallelizable. But memory complexity is quadratic in the sequence length.</li></ol><div style="text-align:center;margin-bottom:1em;"><img src="'+j+'" width="70%" alt="Three ways of Processing Sequences"><br> Three ways of Processing Sequences</div><h3 id="attention-is-all-you-need" tabindex="-1"><a class="header-anchor" href="#attention-is-all-you-need"><span>Attention is All You Need</span></a></h3><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">Attention is All You Need</a></p><p>A model build only with self-attention layers.</p><p>Layer normalization: Self-attention is giving a set of vectors, and layer normalization does not add communication to the vectors.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+Q+'" width="70%" alt="Attention is All You Need"><br> Attention is All You Need</div><div style="text-align:center;margin-bottom:1em;"><img src="'+Z+'" width="70%" alt="Attention is All You Need"><br> The transformer</div><p>&quot;ImageNet&quot; Moment for Natural Language Processing.</p><p>Pretraining: Download a lot of text from the internet. Train a giant Transformer model for language modeling.</p><p>Fine-tuning: Fine-tune the transformer on your own NLP task.</p><h3 id="scaling-up-transformers" tabindex="-1"><a class="header-anchor" href="#scaling-up-transformers"><span>Scaling up Transformers</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+X+'" width="70%" alt="Scaling up Transformers"><br> Scaling up Transformers</div><h3 id="summary" tabindex="-1"><a class="header-anchor" href="#summary"><span>Summary</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+J+'" width="70%" alt="Summary"><br> Summary</div><h2 id="notice-on-usage-and-attribution" tabindex="-1"><a class="header-anchor" href="#notice-on-usage-and-attribution"><span><strong>Notice on Usage and Attribution</strong></span></a></h2><p>This note is based on the <strong>University of Michigan&#39;s publicly available course EECS 498.008 / 598.008</strong> and is intended <strong>solely for personal learning and academic discussion</strong>, with no commercial use.</p><ul><li><strong>Nature of the Notes:</strong> These notes include extensive references and citations from course materials to ensure clarity and completeness. However, they are presented as personal interpretations and summaries, not as substitutes for the original course content.</li><li><strong>Original Course Resources:</strong> Please refer to the official <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/" target="_blank" rel="noopener noreferrer"><strong>University of Michigan website</strong></a> for complete and accurate course materials.</li><li><strong>Third-Party Open Access Content:</strong> This note may reference Open Access (OA) papers or resources cited within the course materials. These materials are used under their original Open Access licenses (e.g., CC BY, CC BY-SA).</li><li><strong>Proper Attribution:</strong> Every referenced OA resource is appropriately cited, including the author, publication title, source link, and license type.</li><li><strong>Copyright Notice:</strong> All rights to third-party content remain with their respective authors or publishers.</li><li><strong>Content Removal:</strong> If you believe any content infringes on your copyright, please contact me, and I will promptly remove the content in question.</li></ul><p>Thanks to the <strong>University of Michigan</strong> and the contributors to the course for their openness and dedication to accessible education.</p>',119)])])}const ts=n(K,[["render",$]]),ns=JSON.parse('{"path":"/notes/um-cv/um-cv-12/","title":"12 & 13 Recurrent Neural Networks & Attention Mechanism","lang":"en-US","frontmatter":{"title":"12 & 13 Recurrent Neural Networks & Attention Mechanism","tags":["notes","computer-vision"],"createTime":"2024/12/27 11:21:07","permalink":"/notes/um-cv/um-cv-12/","outline":[2,4],"description":"Part 1: RNNs, Vanilla Rnns, LSTMs, GRU, Gradient explosion, Gradient vanishing, Architecture Search, Empirical Understanding of RNNs. Part 2: Seq2Seq, Attention Mechanism, Self-...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"12 & 13 Recurrent Neural Networks & Attention Mechanism\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-21T17:44:17.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://saturntsen.github.io/notes/um-cv/um-cv-12/"}],["meta",{"property":"og:site_name","content":"SaturnTsen"}],["meta",{"property":"og:title","content":"12 & 13 Recurrent Neural Networks & Attention Mechanism"}],["meta",{"property":"og:description","content":"Part 1: RNNs, Vanilla Rnns, LSTMs, GRU, Gradient explosion, Gradient vanishing, Architecture Search, Empirical Understanding of RNNs. Part 2: Seq2Seq, Attention Mechanism, Self-..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-02-21T17:44:17.000Z"}],["meta",{"property":"article:tag","content":"computer-vision"}],["meta",{"property":"article:tag","content":"notes"}],["meta",{"property":"article:modified_time","content":"2025-02-21T17:44:17.000Z"}]]},"readingTime":{"minutes":8,"words":2399},"git":{"createdTime":1735311105000,"updatedTime":1740159857000,"contributors":[{"name":"SaturnTsen","username":"SaturnTsen","email":"minger233@outlook.com","commits":6,"avatar":"https://avatars.githubusercontent.com/SaturnTsen?v=4","url":"https://github.com/SaturnTsen"}]},"autoDesc":true,"filePathRelative":"notes/UM-CV/UM-CV 12 Recurrent Neural Networks & Attention.md","headers":[],"categoryList":[{"id":"4358b5","sort":10000,"name":"notes"},{"id":"31a781","sort":10004,"name":"UM-CV"}]}');export{ts as comp,ns as data};
