import{_ as t,c as o,a as i,o as n}from"./app-BPmJL-vo.js";const a="/images/um-cv-2/15-1.png",r="/images/um-cv-2/15-2.png",s="/images/um-cv-2/15-3.png",c="/images/um-cv-2/15-4.png",l="/images/um-cv-2/15-5.png",p="/images/um-cv-2/15-6.png",g="/images/um-cv-2/15-7.png",m="/images/um-cv-2/15-8.png",d="/images/um-cv-2/15-16.png",h="/images/um-cv-2/15-9.png",u="/images/um-cv-2/15-10.png",b="/images/um-cv-2/15-11.png",f="/images/um-cv-2/15-12.png",v="/images/um-cv-2/15-13.png",N="/images/um-cv-2/15-14.png",C="/images/um-cv-2/15-17.png",w="/images/um-cv-2/15-15.png",x={};function y(R,e){return n(),o("div",null,[...e[0]||(e[0]=[i('<h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary"><span>Summary</span></a></h2><p>Slow R-CNN, Fast R-CNN, Faster R-CNN, Single-Stage Object Detection, and their comparison.</p><p>@Credits: <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/" target="_blank" rel="noopener noreferrer">EECS 498.007</a> | Video Lecture: <a href="https://www.youtube.com/watch?v=dJYGatp4SvA&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r" target="_blank" rel="noopener noreferrer">UM-CV</a></p><p>Personal work for the assignments of the course: <a href="https://github.com/SaturnTsen/EECS-498-007/" target="_blank" rel="noopener noreferrer">github repo</a>.</p><p><strong>Notice on Usage and Attribution</strong></p><p>These are personal class notes based on the University of Michigan EECS 498.008 / 598.008 course. They are intended solely for personal learning and academic discussion, with no commercial use.</p><p>For detailed information, please refer to the <strong><a href="#notice-on-usage-and-attribution">complete notice at the end of this document</a></strong></p><h2 id="intro" tabindex="-1"><a class="header-anchor" href="#intro"><span>Intro</span></a></h2><h3 id="task-definition" tabindex="-1"><a class="header-anchor" href="#task-definition"><span>Task Definition</span></a></h3><p>Input: Single RGB image Outputï¼š A set of detected objects; for each object predict:</p><p>Computer Vision Tasks:Classification, Semantic Segmentation, Object Detection, Instance Segmentation</p><ol><li>Category label (from fixed, known set of categories)</li><li>Bounding box (four numbers, x, y, width, height)</li></ol><h3 id="challenges" tabindex="-1"><a class="header-anchor" href="#challenges"><span>Challenges</span></a></h3><ul><li>Multiple outputs: Need to output variable numbers of objects per image</li><li>Multiple types of output&quot; Need to predict &quot;what&quot; (category label) as well as &quot;where&quot; (bounding box)</li><li>Large images: Classification works at 224 x 224; need higher resolution for detection, often 800 x 800 or more</li><li>Detecting Multiple Objects: Need different numbers of outputs per image</li></ul><h2 id="region-based-cnn-r-cnn" tabindex="-1"><a class="header-anchor" href="#region-based-cnn-r-cnn"><span>Region-based CNN (R-CNN)</span></a></h2><p>CVPR 2014</p><h3 id="window-detection" tabindex="-1"><a class="header-anchor" href="#window-detection"><span>Window Detection</span></a></h3><h4 id="sliding-window" tabindex="-1"><a class="header-anchor" href="#sliding-window"><span>Sliding Window?</span></a></h4><ul><li><p>Apply a cnn to many different crops of the image, CNN classifies each crop as object or background. And enumerate all possible crops -&gt; too slow.</p></li><li><p>Region Proposals: Find a small set of boxes that are likely to cover all objects. Often based on heuristics: e.g. look for &quot;blob-like&quot; image regions. This is relatively fast to run; e.g. Selective Search gives 2000 region proposals in a few seconds on CPU</p></li><li><p>Regions of interest (RoI) from a proposal methods. For each RoI, apply a CNN to predict the class of the object in the RoI and refine the bounding box.</p></li><li><p>Bounding box regression: predict &quot;transform&quot; to correct the RoI: x, y, w, h</p></li></ul><div style="text-align:center;margin-bottom:1em;"><img src="'+a+'" width="70%" alt="Bounding box regression"><br> Fig: Bounding-box regression</div><div style="text-align:center;margin-bottom:1em;"><img src="'+r+'" width="70%" alt="R-CNN"><br> Fig: R-CNN</div><p>During training, backpropagate on all the regions of interest (RoIs).</p><h3 id="comparing-boxes-intersection-over-union-iou" tabindex="-1"><a class="header-anchor" href="#comparing-boxes-intersection-over-union-iou"><span>Comparing Boxes: Intersection over Union (IoU)</span></a></h3><ul><li>IoU = Area of overlap / Area of union. Also called &quot;Jaccard similarity&quot; or &quot;Jaccard index&quot;.</li><li>IoU &gt; 0.5 is &quot;decent&quot;. IoU &gt; 0.7 is &quot;good&quot;. IoU &gt; 0.9 is &quot;almost perfect&quot;.</li></ul><h4 id="overlapping-boxes" tabindex="-1"><a class="header-anchor" href="#overlapping-boxes"><span>Overlapping Boxes</span></a></h4><p>Object detectors often output many overlapping detections. Solution: Post-process raw detections using Non-Max Suppression (NMS).</p><ol><li>Select next highest-scoring box</li><li>Eliminate lower-scoring boxes with IoU &gt; threshold</li></ol><div style="text-align:center;margin-bottom:1em;"><img src="'+s+'" width="70%" alt="Non-Max Suppression"><br> Fig: Non-Max Suppression</div><p>Problem: NMS may eliminate &quot;good&quot; boxes when objects are highly overlapping...</p><h3 id="evaluating-object-detectors-mean-average-precision-map" tabindex="-1"><a class="header-anchor" href="#evaluating-object-detectors-mean-average-precision-map"><span>Evaluating Object Detectors: Mean Average Precision (mAP)</span></a></h3><ol><li>Run object detector on all test images (with NMS)</li><li>For each category, compute average precision (AP) = area under precision vs recall curve</li></ol><div style="text-align:center;margin-bottom:1em;"><img src="'+c+'" width="70%" alt="Mean Average Precision"><br> Fig: Mean Average Precision</div><ol start="3"><li>mean Average Precision (mAP) = mean of AP over all categories</li><li>for &quot;COCO mAP&quot;, average over 10 IoU thresholds (0.5 to 0.95) and take average</li></ol><div style="text-align:center;margin-bottom:1em;"><img src="'+l+'" width="70%" alt="COCO mAP"><br> Fig: COCO mAP</div><h2 id="fast-rcnn" tabindex="-1"><a class="header-anchor" href="#fast-rcnn"><span>Fast-RCNN</span></a></h2><p>ICCV 2015</p><div style="text-align:center;margin-bottom:1em;"><img src="'+p+'" width="70%" alt="Fast-RCNN"><br> Fig: Fast-RCNN</div><h3 id="crop-features-roi-pool" tabindex="-1"><a class="header-anchor" href="#crop-features-roi-pool"><span>Crop Features: RoI Pool</span></a></h3><p>Input Image: e.g. 3 x 640 x 480 -&gt; CNN (e.g. 512 x 20 x15)</p><p>Project and snap RoI to CNN feature map - &gt; Divide into 2x2 grid of (roughly) equal subregions -&gt; Max pool within each subregion (e.g. 512 x 7 x 7)</p><p>Region features always the same size even if input regions have different sizes!</p><div style="text-align:center;margin-bottom:1em;"><img src="'+g+'" width="70%" alt="RoI Pool"><br> Fig: RoI Pool</div><p>Problem: Slight misalignment between RoI and CNN grid can cause misalignment in RoI Pooling. Solution: RoI Align</p><h3 id="roi-align" tabindex="-1"><a class="header-anchor" href="#roi-align"><span>RoI Align</span></a></h3><ul><li>Instead of snapping RoI to CNN grid, interpolate between grid points</li><li>More accurate, but more expensive</li></ul><div style="text-align:center;margin-bottom:1em;"><img src="'+m+'" width="70%" alt="RoI Align"><br> Fig: RoI Align</div><p>The cropping may not perfectly match the original object grid. RoI Align implements a bilinear near-neighbor interpolation to get more accurate cropping. We can consider the image as a real-valued tensor and backpropagate to any points in the image.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+d+'" width="70%" alt="RoI Align"><br> Fig: RoI Align</div><h3 id="fast-r-cnn-vs-slow-r-cnn-iccv-2015" tabindex="-1"><a class="header-anchor" href="#fast-r-cnn-vs-slow-r-cnn-iccv-2015"><span>Fast R-CNN vs &quot;Slow&quot; R-CNN (ICCV 2015)</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+h+'" width="70%" alt="Fast R-CNN"><br> Fig: Fast R-CNN</div><h2 id="faster-r-cnn-learnable-region-proposals" tabindex="-1"><a class="header-anchor" href="#faster-r-cnn-learnable-region-proposals"><span>Faster R-CNN: Learnable Region Proposals</span></a></h2><p>Train a CNN to predict region proposals. NeurIPS 2015</p><div style="text-align:center;margin-bottom:1em;"><img src="'+u+'" width="70%" alt="Faster R-CNN"><br> Fig: Faster R-CNN</div><p><strong>Region Proposal Network (RPN):</strong></p><div style="text-align:center;margin-bottom:1em;"><img src="'+b+'" width="70%" alt="Region Proposal Network"><br> Fig: Region Proposal Network</div><ul><li><p>Anchor box of fixed size at each point in the feature map.</p></li><li><p>At each point, predict whether the corresponding anchor contains an object or not (per-cell logistic regression, predict scores with conv layer.)</p></li><li><p>For positive boxes, also predict a box transform to regress from anchor to object box.</p></li><li><p>Problem: Anchor box may be too small or too large for the object. Solution: K Multi-scale anchors for each point.</p></li></ul><div style="text-align:center;margin-bottom:1em;"><img src="'+f+'" width="70%" alt="Multi-scale anchors"><br> Fig: Multi-scale anchors</div><p>Jointly with 4 losses:</p><ol><li>RPN classification loss: anchor box is object or not</li><li>RPN regression loss: predict transform from anchor box to proposal box</li><li>Object classification: classify proposals as background or object class</li><li>Object regression: predict transform from a proposal box to object box</li></ol><div style="text-align:center;margin-bottom:1em;"><img src="'+v+'" width="50%" alt="Faster R-CNN"><br> Fig: Faster R-CNN</div><p>Two stages:</p><p>First stage: Run once per image</p><ul><li>Backbone Network</li><li>Region proposal network</li></ul><p>Second stage: Run once per region</p><ul><li>Crop features: RoI pool/align</li><li>Predict object class</li><li>Prediction bbox offset</li></ul><h2 id="single-stage-object-detection" tabindex="-1"><a class="header-anchor" href="#single-stage-object-detection"><span>Single-Stage Object Detection</span></a></h2><ul><li>YOLO: You Only Look Once. ECCV 2016</li><li>Focal Loss for Dense Object Detection. ICCV 2017</li></ul><div style="text-align:center;margin-bottom:1em;"><img src="'+N+'" width="70%" alt="YOLO"><br> Fig: YOLO</div><h2 id="detection-without-anchors-cornernet-eccv-2018" tabindex="-1"><a class="header-anchor" href="#detection-without-anchors-cornernet-eccv-2018"><span>Detection without Anchors: CornerNet (ECCV 2018)</span></a></h2><p>Use a backbone CNN to predict the heatmap of object upper-left corners and lower-right corners.</p><p>To match the upper-left and lower-right corners, use a &quot;associative embedding&quot; to predict the offset between the two corners.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+C+'" width="70%" alt="CornerNet"><br> Fig: CornerNet</div><h2 id="comparison" tabindex="-1"><a class="header-anchor" href="#comparison"><span>Comparison</span></a></h2><p><a href="https://arxiv.org/abs/1611.10012" target="_blank" rel="noopener noreferrer">Speed/accuracy trade-offs for modern convolutional object detectors</a> CVPR 2017</p><p>Takeaways:</p><ul><li>Two stage method (e.g. Faster R-CNN) is more accurate but slower</li><li>Single stage methods (e.g. YOLO, SSD) are faster but less accurate</li><li>Bigger backbones improve performance, but are slower</li><li>Nowadays, single stage methods are as good as two-stage methods</li><li>Very big models work better</li><li>Test-time augmentation pushes numbers up</li><li>Big ensembles, more data, etc</li></ul><div style="text-align:center;margin-bottom:1em;"><img src="'+w+'" width="70%" alt="Comparison"><br> Fig: Comparison</div><h3 id="object-detection-open-source-code" tabindex="-1"><a class="header-anchor" href="#object-detection-open-source-code"><span>Object Detection: Open-Source Code</span></a></h3><p>Don&#39;t implement it yourself (Unless you are working on the assignment)</p><p><a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener noreferrer">Detectron2 (PyTorch)</a></p><p>Fast/Faster/Mask R-CNN, RetinaNet</p><h2 id="notice-on-usage-and-attribution" tabindex="-1"><a class="header-anchor" href="#notice-on-usage-and-attribution"><span><strong>Notice on Usage and Attribution</strong></span></a></h2><p>This note is based on the <strong>University of Michigan&#39;s publicly available course EECS 498.008 / 598.008</strong> and is intended <strong>solely for personal learning and academic discussion</strong>, with no commercial use.</p><ul><li><strong>Nature of the Notes:</strong> These notes include extensive references and citations from course materials to ensure clarity and completeness. However, they are presented as personal interpretations and summaries, not as substitutes for the original course content.</li><li><strong>Original Course Resources:</strong> Please refer to the official <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/" target="_blank" rel="noopener noreferrer"><strong>University of Michigan website</strong></a> for complete and accurate course materials.</li><li><strong>Third-Party Open Access Content:</strong> This note may reference Open Access (OA) papers or resources cited within the course materials. These materials are used under their original Open Access licenses (e.g., CC BY, CC BY-SA).</li><li><strong>Proper Attribution:</strong> Every referenced OA resource is appropriately cited, including the author, publication title, source link, and license type.</li><li><strong>Copyright Notice:</strong> All rights to third-party content remain with their respective authors or publishers.</li><li><strong>Content Removal:</strong> If you believe any content infringes on your copyright, please contact me, and I will promptly remove the content in question.</li></ul><p>Thanks to the <strong>University of Michigan</strong> and the contributors to the course for their openness and dedication to accessible education.</p>',85)])])}const _=t(x,[["render",y]]),j=JSON.parse('{"path":"/notes/um-cv/um-cv-15/","title":"15 Object Detection","lang":"en-US","frontmatter":{"title":"15 Object Detection","createTime":"2024/12/29 09:10:12","tags":["computer-vision"],"permalink":"/notes/um-cv/um-cv-15/","outline":[2,4],"description":"Summary Slow R-CNN, Fast R-CNN, Faster R-CNN, Single-Stage Object Detection, and their comparison. @Credits: EECS 498.007 | Video Lecture: UM-CV Personal work for the assignment...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"15 Object Detection\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-21T17:44:17.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://saturntsen.github.io/notes/um-cv/um-cv-15/"}],["meta",{"property":"og:site_name","content":"SaturnTsen"}],["meta",{"property":"og:title","content":"15 Object Detection"}],["meta",{"property":"og:description","content":"Summary Slow R-CNN, Fast R-CNN, Faster R-CNN, Single-Stage Object Detection, and their comparison. @Credits: EECS 498.007 | Video Lecture: UM-CV Personal work for the assignment..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-02-21T17:44:17.000Z"}],["meta",{"property":"article:tag","content":"computer-vision"}],["meta",{"property":"article:modified_time","content":"2025-02-21T17:44:17.000Z"}]]},"readingTime":{"minutes":5.38,"words":1615},"git":{"createdTime":1735477870000,"updatedTime":1740159857000,"contributors":[{"name":"SaturnTsen","username":"SaturnTsen","email":"minger233@outlook.com","commits":5,"avatar":"https://avatars.githubusercontent.com/SaturnTsen?v=4","url":"https://github.com/SaturnTsen"}]},"autoDesc":true,"filePathRelative":"notes/UM-CV/UM-CV 15 Object Detection.md","headers":[],"categoryList":[{"id":"4358b5","sort":10000,"name":"notes"},{"id":"31a781","sort":10004,"name":"UM-CV"}]}');export{_ as comp,j as data};
