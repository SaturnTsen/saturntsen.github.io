import{_ as a,c as e,a as t,o as n}from"./app-BPmJL-vo.js";const i="/images/um-cv-2/17-1.png",r="/images/um-cv-2/17-2.png",p={};function l(o,s){return n(),e("div",null,[...s[0]||(s[0]=[t('<p>Topics in 3D vision: Computing correspondences, stereo, structure from motion, simultaneous localization and mapping(SLAM), view synthesis, differentiable graphics, etc.</p><p>Focus: 3D shape prediction - Shape representations, shape comparison metrics, camera systems, coordinates, datasets, and an example of Mesh R-CNN.</p><p>@Credits: <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/" target="_blank" rel="noopener noreferrer">EECS 498.007</a> | Video Lecture: <a href="https://www.youtube.com/watch?v=dJYGatp4SvA&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r" target="_blank" rel="noopener noreferrer">UM-CV</a></p><p>Personal work for the assignments of the course: <a href="https://github.com/SaturnTsen/EECS-498-007/" target="_blank" rel="noopener noreferrer">github repo</a>.</p><p><strong>Notice on Usage and Attribution</strong></p><p>These are personal class notes based on the University of Michigan EECS 498.008 / 598.008 course. They are intended solely for personal learning and academic discussion, with no commercial use.</p><p>For detailed information, please refer to the <strong><a href="#notice-on-usage-and-attribution">complete notice at the end of this document</a></strong></p><h2 id="shape-representations" tabindex="-1"><a class="header-anchor" href="#shape-representations"><span>Shape representations</span></a></h2><h3 id="depth-map" tabindex="-1"><a class="header-anchor" href="#depth-map"><span>Depth map</span></a></h3><p>Depth map assigns a depth value to each pixel in the image. RGB-D image.</p><ul><li>Cannot capture occluded regions. Data can be recorded directly for some types of 3D sensors.</li><li>Task: Predicting Depth Maps</li><li>Problem: Scale/Depth Ambiguity</li><li>Solution: Use a loss function that is invariant to scale, e.g. log scale.</li></ul><h3 id="surfacer-normals" tabindex="-1"><a class="header-anchor" href="#surfacer-normals"><span>Surfacer Normals</span></a></h3><p>This gives a vector giving the normal vector to the object in the world for that pixel. Ground-truth normals: 3 × H × W</p><h3 id="voxel-grid" tabindex="-1"><a class="header-anchor" href="#voxel-grid"><span>Voxel grid</span></a></h3><p>Represent a shape with a V × V × V grid of occupancies. Just like segmentation masks in Mask R-CNN, but in 3D.</p><ul><li>Conceptually simple</li><li>High resolution to capture fine structures</li><li>Scaling to high resolutions is nontrivial</li><li>Processing Voxel Inputs: 3D Convolution similar</li><li>Generating Voxel Shapes from 2D Images <ul><li>2D image -&gt; 2D CNN -&gt; FC -&gt; 3D CNN -&gt; Voxel grid</li><li>Problem: 3D Convolution is computationally expensive, also memory intensive. e.g. storing a 1024^3 voxel grid requires 4GB of memory.</li><li>Voxel Tubes <a href="https://arxiv.org/abs/1604.00449" target="_blank" rel="noopener noreferrer">3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction, ECCV 2016</a>: Final conv layer: V filters Interpret as a &quot;tube&quot; of voxel scores. Problem: loss of translation invariance in z direction.</li><li>Scaling Voxel Networks: Octree-based methods <a href="https://arxiv.org/abs/1611.05009" target="_blank" rel="noopener noreferrer">OctNet: Learning Deep 3D Representations at High Resolutions, ICCV 2017</a></li><li>Nested Shape Layers <a href="https://arxiv.org/abs/1804.10975" target="_blank" rel="noopener noreferrer">Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers, CVPR 2018</a></li></ul></li></ul><h3 id="implicit-surface" tabindex="-1"><a class="header-anchor" href="#implicit-surface"><span>Implicit Surface</span></a></h3><p>Learn a function to classify arbitrary 3D points as inside/outside the shape. Signed Distance Function (SDF) gives the Euclidean distance to the surface of the shape; sign gives inside/outside</p><h3 id="point-cloud" tabindex="-1"><a class="header-anchor" href="#point-cloud"><span>Point cloud</span></a></h3><p>A set of points in 3D space.</p><ul><li>(+) Can represent fine structures without huge numbers of points</li><li>( ) Requires new architectures, losses, etc</li><li>(-) Doesn&#39;t explicitly represent the surface of the shape: extracting a mesh for rendering or other. Need postprocessing to transform into another format.</li><li>Segmentation: PointNet <a href="https://arxiv.org/abs/1612.00593" target="_blank" rel="noopener noreferrer">PointNet, CVPR 2017</a></li><li>Generating Point cloud Outputs: <a href="https://arxiv.org/abs/1612.00603" target="_blank" rel="noopener noreferrer">A Point Set Generation Network for 3D Object Reconstruction from a Single Image, CVPR 2017</a></li><li>Chamfer Distance: Measure the distance between two point clouds. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>C</mi><mi>D</mi></mrow></msub><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></msub><msub><mrow><mi>min</mi><mo>⁡</mo></mrow><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></msub><mi mathvariant="normal">∥</mi><mi>x</mi><mo>−</mo><mi>y</mi><msup><mi mathvariant="normal">∥</mi><mn>2</mn></msup><mo>+</mo><msub><mo>∑</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></msub><msub><mrow><mi>min</mi><mo>⁡</mo></mrow><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></msub><mi mathvariant="normal">∥</mi><mi>x</mi><mo>−</mo><mi>y</mi><msup><mi mathvariant="normal">∥</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">d_{CD}(X, Y) = \\sum_{x \\in X} \\min_{y \\in Y} \\|x - y\\|^2 + \\sum_{y \\in Y} \\min_{x \\in X} \\|x - y\\|^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0771em;vertical-align:-0.3271em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1786em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3271em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop">min</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">Y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∥</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1858em;vertical-align:-0.4358em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1786em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">Y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop">min</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∥</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></li></ul><h3 id="mesh" tabindex="-1"><a class="header-anchor" href="#mesh"><span>Mesh</span></a></h3><p>Triangle mesh:</p><ul><li>Vertices: Set of V points in 3D space</li><li>Faces: Set of triangles over the Vertices</li><li>(+) Standard representation for graphics</li><li>(-) Requires new architectures of neural Networks</li></ul><p>Pixel2Mesh <a href="https://arxiv.org/abs/1804.01654" target="_blank" rel="noopener noreferrer">Generating 3D Mesh Models from Single RGB Images, ECCV 2018</a></p><p>Key ideas:</p><ol><li><strong>idea 1</strong> Iterative mesh refinement <ul><li>Start from initial ellipsoid mesh network predicts offsets for each vertex repeat.</li><li>Predicting Triangle Meshes: Graph convolution</li><li>Input: Graph with a feature vector at each vertex.</li><li>Output: New feature vector for each vertex. <ul><li>Graph Convolution: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>v</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mi>σ</mi><mrow><mo fence="true">(</mo><msub><mo>∑</mo><mrow><mi>u</mi><mo>∈</mo><mi>N</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></msub><mfrac><mn>1</mn><msub><mi>c</mi><mi>v</mi></msub></mfrac><msup><mi>W</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup><msubsup><mi>h</mi><mi>u</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msubsup><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">h_v^{(l+1)} = \\sigma \\left( \\sum_{u \\in N(v)} \\frac{1}{c_v} W^{(l)} h_u^{(l)} \\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1614em;vertical-align:-0.1166em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.5834em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1166em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.8em;vertical-align:-0.65em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2253em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4747em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4451em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.5834em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">u</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1166em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span></span></span></span></li></ul></li></ul></li><li><strong>idea 2:</strong> How to incorporate image information? <ul><li>Aligned vertex features</li><li>For each vertex of the mesh, use camera information fo <strong>project</strong> onto image plane.</li><li>Use a bilinear interpolation to sample a CNN feature</li></ul></li><li><strong>Loss function</strong><ul><li>Prediction &amp; Ground-truth is not unique, so we want our prediction to be invariant to the representation.</li><li>Loss: Chamfer distance between predicted samples and ground-truth samples.</li><li><a href="https://arxiv.org/abs/1901.11461" target="_blank" rel="noopener noreferrer">GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects, ICML 2019</a></li></ul></li></ol><h2 id="shape-comparison-metrics" tabindex="-1"><a class="header-anchor" href="#shape-comparison-metrics"><span>Shape Comparison Metrics</span></a></h2><h3 id="iou-over-3d-shapes" tabindex="-1"><a class="header-anchor" href="#iou-over-3d-shapes"><span>IoU over 3D shapes</span></a></h3><h3 id="chamfer-distance" tabindex="-1"><a class="header-anchor" href="#chamfer-distance"><span>Chamfer distance</span></a></h3><ul><li>Problem: Very sensitive to outliers</li></ul><h3 id="f1-score" tabindex="-1"><a class="header-anchor" href="#f1-score"><span>F1 Score</span></a></h3><ul><li>Precision@t = fraction of predicted points within distance t of some ground-truth point.</li><li>Recall@t = fraction of ground-truth points within distance t of some predicted point.</li><li><a href="https://arxiv.org/abs/1904.04514" target="_blank" rel="noopener noreferrer">What Do single-view 3D Reconstruction Networks Learn? CVPR 2019</a></li></ul><div class="img-wrapper"><img src="'+i+'" width="50%" alt="description"><br> Fig: F1 score</div><h2 id="camera-systems-coordinates" tabindex="-1"><a class="header-anchor" href="#camera-systems-coordinates"><span>Camera Systems, Coordinates</span></a></h2><p><strong>Canonical Coordinates</strong>: Predict 3D shape in a canonical coordinate system(e.g. front of chair is +z) regardless of the viewpoint of the camera.</p><p><strong>View Coordinates</strong>: Predict 3D shape aligned to the viewpoint of the camera.</p><p>Many papers predict in canonical coordinates - easier to load data. However, canonical view breaks the &quot;principle of feature alignment&quot;: Predictions should be aligned to inputs.</p><p><a href="https://arxiv.org/abs/1804.06032" target="_blank" rel="noopener noreferrer">Pixels, voxels and views: A study of shape representations for single view 3D object shape prediction, CVPR 2019</a></p><p>Idea: View-Centric Voxel Predictions</p><h2 id="datasets" tabindex="-1"><a class="header-anchor" href="#datasets"><span>Datasets</span></a></h2><p>3D Datasets: Object-centric. ShapeNet. ~50 categories, ~50k 3D CAD models.</p><p>Pix3D: Some papers train on ShapeNet and show qualitative results here, but use ground-truth segmentation masks. IKEA furniture aligned to ~17k images.</p><h2 id="example" tabindex="-1"><a class="header-anchor" href="#example"><span>Example</span></a></h2><h3 id="mesh-r-cnn" tabindex="-1"><a class="header-anchor" href="#mesh-r-cnn"><span>Mesh R-CNN</span></a></h3><p><a href="https://arxiv.org/abs/1906.02739" target="_blank" rel="noopener noreferrer">Mesh R-CNN, ICCV 2019</a></p><p>Input: Single RGB image</p><p>Output: A set of detected objects for each object:</p><p>Mask R-CNN</p><ul><li>bbox</li><li>category label</li><li>instance segmentation</li></ul><p>Mesh head</p><ul><li>3d triangle mesh</li></ul><p><strong>Problem of Mesh deformation</strong> The topology is fixed by the initial mesh - All predictions are topologically equivalent to the initial mesh.</p><p><strong>Mesh R-CNN Pipeline</strong></p><div class="img-wrapper"><img src="'+r+'" width="50%" alt="description"><br> Fig: Mesh R-CNN Pipeline</div><p>Add <strong>shape regularizers</strong> to the loss function to encourage the predicted mesh to be regular and smooth.</p><p>Concept: <strong>Amodal completion</strong> - predict occluded parts of the objects.</p><h2 id="notice-on-usage-and-attribution" tabindex="-1"><a class="header-anchor" href="#notice-on-usage-and-attribution"><span><strong>Notice on Usage and Attribution</strong></span></a></h2><p>This note is based on the <strong>University of Michigan&#39;s publicly available course EECS 498.008 / 598.008</strong> and is intended <strong>solely for personal learning and academic discussion</strong>, with no commercial use.</p><ul><li><strong>Nature of the Notes:</strong> These notes include extensive references and citations from course materials to ensure clarity and completeness. However, they are presented as personal interpretations and summaries, not as substitutes for the original course content.</li><li><strong>Original Course Resources:</strong> Please refer to the official <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/" target="_blank" rel="noopener noreferrer"><strong>University of Michigan website</strong></a> for complete and accurate course materials.</li><li><strong>Third-Party Open Access Content:</strong> This note may reference Open Access (OA) papers or resources cited within the course materials. These materials are used under their original Open Access licenses (e.g., CC BY, CC BY-SA).</li><li><strong>Proper Attribution:</strong> Every referenced OA resource is appropriately cited, including the author, publication title, source link, and license type.</li><li><strong>Copyright Notice:</strong> All rights to third-party content remain with their respective authors or publishers.</li><li><strong>Content Removal:</strong> If you believe any content infringes on your copyright, please contact me, and I will promptly remove the content in question.</li></ul><p>Thanks to the <strong>University of Michigan</strong> and the contributors to the course for their openness and dedication to accessible education.</p>',61)])])}const c=a(p,[["render",l]]),h=JSON.parse('{"path":"/notes/um-cv/um-cv-17/","title":"17 3D Vision","lang":"en-US","frontmatter":{"title":"17 3D Vision","tags":["computer-vision"],"createTime":"2025/01/04 23:09:39","permalink":"/notes/um-cv/um-cv-17/","outline":[2,4],"description":"Topics in 3D vision: Computing correspondences, stereo, structure from motion, simultaneous localization and mapping(SLAM), view synthesis, differentiable graphics, etc. Focus: ...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"17 3D Vision\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-21T17:44:17.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://saturntsen.github.io/notes/um-cv/um-cv-17/"}],["meta",{"property":"og:site_name","content":"SaturnTsen"}],["meta",{"property":"og:title","content":"17 3D Vision"}],["meta",{"property":"og:description","content":"Topics in 3D vision: Computing correspondences, stereo, structure from motion, simultaneous localization and mapping(SLAM), view synthesis, differentiable graphics, etc. Focus: ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-02-21T17:44:17.000Z"}],["meta",{"property":"article:tag","content":"computer-vision"}],["meta",{"property":"article:modified_time","content":"2025-02-21T17:44:17.000Z"}]]},"readingTime":{"minutes":4.05,"words":1215},"git":{"createdTime":1736090482000,"updatedTime":1740159857000,"contributors":[{"name":"SaturnTsen","username":"SaturnTsen","email":"minger233@outlook.com","commits":4,"avatar":"https://avatars.githubusercontent.com/SaturnTsen?v=4","url":"https://github.com/SaturnTsen"}]},"autoDesc":true,"filePathRelative":"notes/UM-CV/UM-CV 17 3D Vision.md","headers":[],"categoryList":[{"id":"4358b5","sort":10000,"name":"notes"},{"id":"31a781","sort":10004,"name":"UM-CV"}]}');export{c as comp,h as data};
