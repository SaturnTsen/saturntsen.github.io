import{_ as t,c as n,a,d as s,o as i}from"./app-BPmJL-vo.js";const l="/images/um-cv/10-1.png",p="/images/um-cv/10-2.png",r="/images/um-cv/10-3.png",m="/images/um-cv/10-4.png",o="/images/um-cv/10-5.png",c="/images/um-cv/10-6.png",h="/images/um-cv/10-7.png",g="/images/um-cv/10-8.png",d="/images/um-cv/10-9.png",u="/images/um-cv/10-10.png",y="/images/um-cv/10-11.png",v="/images/um-cv/10-12.png",x="/images/um-cv/10-13.png",f="/images/um-cv/10-14.png",w="/images/um-cv/10-15.png",b="/images/um-cv/10-16.png",k="/images/um-cv/10-17.png",z="/images/um-cv/10-18.png",L="/images/um-cv/10-19.png",_="/images/um-cv/11-1.png",T="/images/um-cv/11-2.png",M="/images/um-cv/11-3.png",R="/images/um-cv/11-4.png",S="/images/um-cv/11-5.png",U="/images/um-cv/11-6.png",C="/images/um-cv/11-7.png",N="/images/um-cv/11-8.png",A="/images/um-cv/11-9.png",F="/images/um-cv/11-10.png",W="/images/um-cv/11-11.png",E="/images/um-cv/11-12.png",D="/images/um-cv/11-13.png",P="/images/um-cv/11-14.png",q="/images/um-cv/11-15.png",I="/images/um-cv/11-16.png",G={};function O(B,e){return i(),n("div",null,[...e[0]||(e[0]=[a('<p>Summary: Training neural networks, activation functions, data preprocessing, weight initialization, regularization, learning rate schedules, large batch training, hyperparameter tuning, model ensembles, transfer learning.</p><p>@Credits: <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/" target="_blank" rel="noopener noreferrer">EECS 498.007</a> | Video Lecture: <a href="https://www.youtube.com/watch?v=dJYGatp4SvA&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r" target="_blank" rel="noopener noreferrer">UM-CV</a></p><p>Personal work for the assignments of the course: <a href="https://github.com/SaturnTsen/EECS-498-007/" target="_blank" rel="noopener noreferrer">github repo</a>.</p><p><strong>Notice on Usage and Attribution</strong></p><p>These are personal class notes based on the University of Michigan EECS 498.008 / 598.008 course. They are intended solely for personal learning and academic discussion, with no commercial use.</p><p>For detailed information, please refer to the <strong><a href="#notice-on-usage-and-attribution">complete notice at the end of this document</a></strong></p><h2 id="one-time-setup" tabindex="-1"><a class="header-anchor" href="#one-time-setup"><span>One-time setup</span></a></h2><p>Activation functions, data preprocessing, weight initialization, regularization</p><h3 id="activation-functions" tabindex="-1"><a class="header-anchor" href="#activation-functions"><span>Activation functions</span></a></h3><p>Activation functions adds critical linearity for neural networks</p><h4 id="sigmoid" tabindex="-1"><a class="header-anchor" href="#sigmoid"><span>Sigmoid</span></a></h4><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\\sigma(x) = \\frac{1}{1 + e^{-x}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0908em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6973em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><ul><li>Squashes numbers to range [0,1]</li><li>Historically popular since they have nice interpretation as a starting &quot;firing rate&quot; of a neuron</li></ul><p>3 problems:</p><ol><li>Saturated neurons kill the gradients. (The most problematic aspect)</li><li>Sigmoid outputs are not zero-centered. Suppose a multi-layer network, then the inputs of all the layers are always positive Also the gradient of this function is always positive. All the gradients for the weights will have the same sign, and gradients will always push the weights into the same direction. This becomes less of a problem when using mini-batches.</li></ol><div style="text-align:center;margin-bottom:1em;"><img src="'+l+'" width="80%" alt="Gradient update directions"><br> Fig: Gradient update directions </div><ol start="3"><li>exp() is a bit compute expensive: transcendental function. For GPUs, this is not a big deal, the copying takes more time than the computation.</li></ol><h4 id="tanh" tabindex="-1"><a class="header-anchor" href="#tanh"><span>Tanh</span></a></h4><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac><mo>=</mo><mn>2</mn><mi>σ</mi><mo stretchy="false">(</mo><mn>2</mn><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\\sigma(2x) - 1 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2177em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4483em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5904em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6973em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span></p><ul><li>Squashes numbers to range [-1,1]</li><li>zero centered</li><li>still kills gradients when saturated 😦</li></ul><h4 id="relu" tabindex="-1"><a class="header-anchor" href="#relu"><span>ReLU</span></a></h4><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>ReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\\text{ReLU}(x) = \\max(0, x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">ReLU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><ul><li>Does not saturate in the positive region</li><li>Computationally efficient</li><li>Converges much faster than sigmoid/tanh in practice (e.g. 6x)</li></ul><p>Problems:</p><ul><li>Not zero-centered output</li><li>Dying ReLU problem: neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this case, the gradient flowing through a ReLU neuron is always 0 because the gradient of max(0, x) is 0 if x &lt; 0.</li></ul><div style="text-align:center;margin-bottom:1em;"><img src="'+p+'" width="80%" alt="Dying ReLU problem"><br> Fig: Dying ReLU problem </div><p>dead ReLU will never activate -&gt; never update its weights</p><h4 id="leaky-relu" tabindex="-1"><a class="header-anchor" href="#leaky-relu"><span>Leaky ReLU</span></a></h4><p>Sometimes initialize ReLU neurons with a slightly positive slope in order to mitigate the dying ReLU problem.</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>LeakyReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.01</mn><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\\text{LeakyReLU}(x) = \\max(0.01x, x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">LeakyReLU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0.01</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><ul><li>Does not saturate</li><li>Computationally efficient</li><li>Converges much faster than sigmoid/tanh in practice! (e.g. 6x)</li><li>will not &quot;die&quot;</li></ul><h4 id="parametric-relu-prelu" tabindex="-1"><a class="header-anchor" href="#parametric-relu-prelu"><span>Parametric ReLU (PReLU)</span></a></h4><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>PReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>α</mi><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\\text{PReLU}(x) = \\max(\\alpha x, x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">PReLU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathnormal">αx</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is learned and backpropagated through.</p><h4 id="exponential-linear-units-elu" tabindex="-1"><a class="header-anchor" href="#exponential-linear-units-elu"><span>Exponential Linear Units (ELU)</span></a></h4>',35),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mtext",null,"ELU"),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mrow",null,[s("mo",{fence:"true"},"{"),s("mtable",{rowspacing:"0.36em",columnalign:"left left",columnspacing:"1em"},[s("mtr",null,[s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mi",null,"x")])]),s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mrow",null,[s("mtext",null,"if "),s("mi",null,"x"),s("mo",null,">"),s("mn",null,"0")])])])]),s("mtr",null,[s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mrow",null,[s("mi",null,"α"),s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"e"),s("mi",null,"x")]),s("mo",null,"−"),s("mn",null,"1"),s("mo",{stretchy:"false"},")")])])]),s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mrow",null,[s("mtext",null,"if "),s("mi",null,"x"),s("mo",null,"≤"),s("mn",null,"0")])])])])])])]),s("annotation",{encoding:"application/x-tex"}," \\text{ELU}(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha(e^x - 1) & \\text{if } x \\leq 0 \\end{cases} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord text"},[s("span",{class:"mord"},"ELU")]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"3em","vertical-align":"-1.25em"}}),s("span",{class:"minner"},[s("span",{class:"mopen delimcenter",style:{top:"0em"}},[s("span",{class:"delimsizing size4"},"{")]),s("span",{class:"mord"},[s("span",{class:"mtable"},[s("span",{class:"col-align-l"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.69em"}},[s("span",{style:{top:"-3.69em"}},[s("span",{class:"pstrut",style:{height:"3.008em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x")])]),s("span",{style:{top:"-2.25em"}},[s("span",{class:"pstrut",style:{height:"3.008em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.0037em"}},"α"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"x")])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"−"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mord"},"1"),s("span",{class:"mclose"},")")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.19em"}},[s("span")])])])]),s("span",{class:"arraycolsep",style:{width:"1em"}}),s("span",{class:"col-align-l"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.69em"}},[s("span",{style:{top:"-3.69em"}},[s("span",{class:"pstrut",style:{height:"3.008em"}}),s("span",{class:"mord"},[s("span",{class:"mord text"},[s("span",{class:"mord"},"if ")]),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},">"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mord"},"0")])]),s("span",{style:{top:"-2.25em"}},[s("span",{class:"pstrut",style:{height:"3.008em"}}),s("span",{class:"mord"},[s("span",{class:"mord text"},[s("span",{class:"mord"},"if ")]),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≤"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mord"},"0")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.19em"}},[s("span")])])])])])]),s("span",{class:"mclose nulldelimiter"})])])])])])],-1),a('<ul><li>default <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\\alpha = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></li><li>All benefits of ReLU</li><li>Closer to zero mean outputs</li><li>Negative saturation regime compared with ReLU</li></ul><p>Problem:</p><ul><li>Computationally more expensive</li></ul><h4 id="selu" tabindex="-1"><a class="header-anchor" href="#selu"><span>SELU</span></a></h4><p>A rescaled version of ELU</p>',5),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mtext",null,"SELU"),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mi",null,"λ"),s("mrow",null,[s("mo",{fence:"true"},"{"),s("mtable",{rowspacing:"0.36em",columnalign:"left left",columnspacing:"1em"},[s("mtr",null,[s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mi",null,"x")])]),s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mrow",null,[s("mtext",null,"if "),s("mi",null,"x"),s("mo",null,">"),s("mn",null,"0")])])])]),s("mtr",null,[s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mrow",null,[s("mi",null,"α"),s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"e"),s("mi",null,"x")]),s("mo",null,"−"),s("mn",null,"1"),s("mo",{stretchy:"false"},")")])])]),s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mrow",null,[s("mtext",null,"if "),s("mi",null,"x"),s("mo",null,"≤"),s("mn",null,"0")])])])])])])]),s("annotation",{encoding:"application/x-tex"},"\\text{SELU}(x) = \\lambda \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha(e^x - 1) & \\text{if } x \\leq 0 \\end{cases} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord text"},[s("span",{class:"mord"},"SELU")]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"3em","vertical-align":"-1.25em"}}),s("span",{class:"mord mathnormal"},"λ"),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"minner"},[s("span",{class:"mopen delimcenter",style:{top:"0em"}},[s("span",{class:"delimsizing size4"},"{")]),s("span",{class:"mord"},[s("span",{class:"mtable"},[s("span",{class:"col-align-l"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.69em"}},[s("span",{style:{top:"-3.69em"}},[s("span",{class:"pstrut",style:{height:"3.008em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x")])]),s("span",{style:{top:"-2.25em"}},[s("span",{class:"pstrut",style:{height:"3.008em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.0037em"}},"α"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"x")])])])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"−"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mord"},"1"),s("span",{class:"mclose"},")")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.19em"}},[s("span")])])])]),s("span",{class:"arraycolsep",style:{width:"1em"}}),s("span",{class:"col-align-l"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.69em"}},[s("span",{style:{top:"-3.69em"}},[s("span",{class:"pstrut",style:{height:"3.008em"}}),s("span",{class:"mord"},[s("span",{class:"mord text"},[s("span",{class:"mord"},"if ")]),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},">"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mord"},"0")])]),s("span",{style:{top:"-2.25em"}},[s("span",{class:"pstrut",style:{height:"3.008em"}}),s("span",{class:"mord"},[s("span",{class:"mord text"},[s("span",{class:"mord"},"if ")]),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"≤"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mord"},"0")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.19em"}},[s("span")])])])])])]),s("span",{class:"mclose nulldelimiter"})])])])])])],-1),a('<p>where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1.6732632423543772848170429916717</mn></mrow><annotation encoding="application/x-tex">\\alpha = 1.6732632423543772848170429916717</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1.6732632423543772848170429916717</span></span></span></span></p><p>and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>1.0507009873554804934193349852946</mn></mrow><annotation encoding="application/x-tex">\\lambda = 1.0507009873554804934193349852946</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1.0507009873554804934193349852946</span></span></span></span></p><ul><li>Scaled version of ELU that works better for deep networks</li><li>Self-normalizing property: SELU activations preserve mean and variance of inputs. Can train deep SELU neural networks without normalization layers.</li></ul><h4 id="comparison-of-activation-functions" tabindex="-1"><a class="header-anchor" href="#comparison-of-activation-functions"><span>Comparison of activation functions</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="'+r+'" width="80%" alt="Comparison of activation functions"><br> Fig: Comparison of activation functions </div><p>Summary:</p><ul><li>Don&#39;t think too hard. Just use ReLU.</li><li>Try out Leaky ReLU/ELU/SELU/GELU if you need to squeeze that last 0.1%</li><li>Don&#39;t use sigmoid or tanh. The models will learn very slowly.</li></ul><h3 id="data-preprocessing" tabindex="-1"><a class="header-anchor" href="#data-preprocessing"><span>Data preprocessing</span></a></h3><p>We want to normalize the inputs to have zero mean and unit variance. This helps the model learn faster and prevents the gradients from going out of control.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+m+'" width="80%" alt="Data preprocessing"><br> Fig: Data preprocessing </div><p>In practice, you may also see PCA and Whitening applied to the data.</p><p>Rotate the data so that the principal components are aligned with the axes. This is called PCA whitening. For image data this is not so common.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+o+'" width="80%" alt="PCA whitening"><br> Fig: PCA whitening </div><p>Why do we need to normalize the data?</p><p>Classification loss would be very sensitive to changes in weight matrix; hard to optimize.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+c+'" width="80%" alt="Classification"><br> Fig: Classification</div><h4 id="real-examples" tabindex="-1"><a class="header-anchor" href="#real-examples"><span>Real examples</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="'+h+'" width="60%" alt="Real examples"><br> Fig: Real examples </div><h3 id="weight-initialization" tabindex="-1"><a class="header-anchor" href="#weight-initialization"><span>Weight initialization</span></a></h3><p>Zero initialization is bad because all neurons will have the same gradient and will update in the same way.</p><p>Weight Initialization: small random numbers. Fairly ok with shallow networks, but problems with deeper networks.</p><p>This is to ensure that the gradient behaves nicely at the beginning of training.</p><p>Activation Statistics: Histogram of each layers</p><div style="text-align:center;margin-bottom:1em;"><img src="'+g+'" width="60%" alt="Statistics of a sample x"><br> Fig: Statistics of a sample x </div><p>Problem: This initialization may be too small for deep networks.</p><p>If we initialize the weights too big, all neurons will be saturated and the gradients will be zero.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+d+'" width="70%" alt="Statistics of a sample x"><br> Fig: Statistics of a sample x when weights are too big </div><h4 id="xavier-initialization" tabindex="-1"><a class="header-anchor" href="#xavier-initialization"><span>Xavier initialization</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="'+u+'" width="70%" alt="Xavier initialization"><br> Fig: Xavier initialization </div><p>Derivation: Variance of output = Variance of inputs</p><div style="text-align:center;margin-bottom:1em;"><img src="'+y+`" width="70%" alt="Derivation"><br> Fig: Derivation </div><p>We are only taking about linear layers. For ReLU, this method will collapse to zero again, no learning 😦</p><h4 id="kaiming-msra-initialization" tabindex="-1"><a class="header-anchor" href="#kaiming-msra-initialization"><span>Kaiming/MSRA initialization</span></a></h4><p>Multiply Xavier by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mn>2</mn></msqrt></mrow><annotation encoding="application/x-tex">\\sqrt{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1328em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9072em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord">2</span></span></span><span style="top:-2.8672em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1328em;"><span></span></span></span></span></span></span></span></span> for ReLU activation functions.</p><div style="text-align:center;margin-bottom:1em;"><img src="`+v+'" width="70%" alt="Kaiming/MSRA initialization"><br> Fig: Kaiming/MSRA initialization </div><p>This keeps the variance of the output the same as the variance of the input.</p><p>This is sufficient to got VGG to train from scratch.</p><h4 id="residual-networks" tabindex="-1"><a class="header-anchor" href="#residual-networks"><span>Residual Networks</span></a></h4><p>MSRA is not that useful for residual networks.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+x+'" width="70%" alt="Residual Networks"><br> Fig: Residual Networks </div><h3 id="regularization" tabindex="-1"><a class="header-anchor" href="#regularization"><span>Regularization</span></a></h3><p>Add term to the loss.</p><h3 id="weight-decay" tabindex="-1"><a class="header-anchor" href="#weight-decay"><span>Weight decay</span></a></h3><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo>=</mo><mtext>data loss</mtext><mo>+</mo><mi>λ</mi><mi>R</mi><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L = \\text{data loss} + \\lambda R(W) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord">data loss</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">λ</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span></span></span></span></span></p><p>In common we use:</p><ul><li>L2 regularization: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><msubsup><mi>W</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">R(W) = \\sum_i W_i^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1138em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span> (weight decay)</li><li>L1 regularization: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>W</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">R(W) = \\sum_i |W_i|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span></li><li>Elastic net: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo><mo>=</mo><mi>α</mi><msub><mo>∑</mo><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>W</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mo>+</mo><mi>β</mi><msub><mo>∑</mo><mi>i</mi></msub><msubsup><mi>W</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">R(W) = \\alpha \\sum_i |W_i| + \\beta \\sum_i W_i^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1138em;vertical-align:-0.2997em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span> (L1 + L2)</li></ul><h3 id="dropout" tabindex="-1"><a class="header-anchor" href="#dropout"><span>Dropout</span></a></h3><p>Randomly set some neurons to zero during forward and backward pass.</p><p>We want to prevent the network from relying too much on any one neuron. Prevents co-adaptation of neurons.</p><p>Another interpretation: Ensemble of networks. Dropout is like training an ensemble of networks and averaging their predictions.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+f+'" width="70%" alt="Dropout"><br> Fig: Dropout </div><p>Problem: Test time. We want to use all the neurons.</p><p>We average out the randomness at test-time, but this integral seems hard... So we need to approximate the integral.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+w+'" width="70%" alt="Dropout at test time"><br> Fig: Dropout at test time </div><div style="text-align:center;margin-bottom:1em;"><img src="'+b+'" width="70%" alt="Dropout at test time"><br> Fig: Dropout at test time </div><p>At test time, we multiply the weights by the dropout probability.</p><p>At test time all neurons are active always. =&gt; We must scale the activations so that for each neuron, the expected output is the same as the expected output at training time.</p><p><strong>Drop in forward pass, scale in backward pass.</strong></p><div style="text-align:center;margin-bottom:1em;"><img src="'+k+'" width="70%" alt="Dropout at test time"><br> Fig: Inverted dropout </div><p>Dropout architecture is not common in modern architectures.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+z+'" width="70%" alt="Dropout at test time"><br> Fig: Dropout at test time </div><h3 id="batch-normalization" tabindex="-1"><a class="header-anchor" href="#batch-normalization"><span>Batch normalization</span></a></h3><p>We have already learned this in the previous lecture.</p><p>For ResNet and later, often L2 and batch normalization are the only regularizers!</p><h3 id="data-augmentation" tabindex="-1"><a class="header-anchor" href="#data-augmentation"><span>Data Augmentation</span></a></h3><ul><li>Random crops</li><li>Random flips</li><li>Random scales</li><li>Color jittering</li></ul><div style="text-align:center;margin-bottom:1em;"><img src="'+L+'" width="70%" alt="Data Augmentation"><br> Fig: Data Augmentation </div><p>Get creative for you problem!</p><p>Random mix/combinations of:</p><ul><li>Translation</li><li>Rotation</li><li>Stretching</li><li>Shearing</li><li>lens distortions,... (go crazy)</li></ul><h3 id="other-regularizers" tabindex="-1"><a class="header-anchor" href="#other-regularizers"><span>Other regularizers</span></a></h3><h4 id="dropconnect" tabindex="-1"><a class="header-anchor" href="#dropconnect"><span>DropConnect</span></a></h4><p>Training: Instead of dropping out neurons, drop out weights. Testing: Use all the connections.</p><h4 id="fractional-max-pooling" tabindex="-1"><a class="header-anchor" href="#fractional-max-pooling"><span>Fractional Max Pooling</span></a></h4><p>Training: Randomize the size of the pooling region. Testing: Average predictions over different samples.</p><h4 id="stochastic-depth" tabindex="-1"><a class="header-anchor" href="#stochastic-depth"><span>Stochastic Depth</span></a></h4><p>Training: Skip some residual blocks in ResNet.</p><p>Testing: Use the whole network.</p><h4 id="stochastic-depth-1" tabindex="-1"><a class="header-anchor" href="#stochastic-depth-1"><span>Stochastic Depth</span></a></h4><p>Training: Set random images regions to 0 Testing: Use the whole image</p><h4 id="mixup" tabindex="-1"><a class="header-anchor" href="#mixup"><span>Mixup</span></a></h4><p>Training: Train on random blends of images Testing: Use original images</p><h2 id="training-dynamics" tabindex="-1"><a class="header-anchor" href="#training-dynamics"><span>Training dynamics</span></a></h2><p>learning rate schedules, large batch training, hyperparameter tuning</p><h3 id="learning-rate-schedules" tabindex="-1"><a class="header-anchor" href="#learning-rate-schedules"><span>Learning rate schedules</span></a></h3><p>Starting with hight learning rate and lower it over time.</p><p>Common schedules:</p><ul><li>Step decay: lower the learning rate by a factor every few fixed epochs.</li></ul><div style="text-align:center;margin-bottom:1em;"><img src="'+_+'" width="70%" alt="Step decay"><br> Fig: Step decay </div> - Cosine: lower the learning rate according to a cosine schedule. This has less hyperparameters than step decay. Often used for CV. <div style="text-align:center;margin-bottom:1em;"><img src="'+T+`" width="70%" alt="Cosine"><br> Fig: Cosine </div><ul><li>Linear: linearly decrease the learning rate over time. Often used for NLP.</li><li>Inverse square root: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>α</mi><msqrt><mi>t</mi></msqrt></mfrac></mrow><annotation encoding="application/x-tex">\\frac{\\alpha}{\\sqrt{t}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2334em;vertical-align:-0.538em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6954em;"><span style="top:-2.5613em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8982em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-2.8582em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1418em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.0037em;">α</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is the initial learning rate and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> is the iteration number.</li><li>Constant: keep the learning rate constant. This works already quite well. A constant schedule is often used for fine-tuning. Adam is already adaptive.</li></ul><p>Tip: <code>torch.zero_grad()</code> is important. If you don&#39;t zero the gradients, the gradients will accumulate.</p><p>Usually for Classification problem, the loss will not explode after a long time, but this is possible for other problems, e.g. Reinforcement Learning. (General experience)</p><h3 id="early-stopping" tabindex="-1"><a class="header-anchor" href="#early-stopping"><span>Early stopping</span></a></h3><p>Stop training the model when accuracy on the validation set decreases, or train for a long time, but always keep track of the model snapshot that worked best on val. Always a good idea to do this!</p><h3 id="choosing-hyperparameters" tabindex="-1"><a class="header-anchor" href="#choosing-hyperparameters"><span>Choosing hyperparameters</span></a></h3><h4 id="grid-search" tabindex="-1"><a class="header-anchor" href="#grid-search"><span>Grid search</span></a></h4><p>Choose several values for each hyperparameter often chosen log-linearly. Evaluate all possible choices on this hyperparameter grid.</p><h4 id="random-search" tabindex="-1"><a class="header-anchor" href="#random-search"><span>Random search</span></a></h4><p>Choose several intervals for each hyperparameter often chosen log-linearly.</p><p>Run many different trails randomly.</p><p>This allow us to sample more hyperparameters along one dimension. This is useful when one hyperparameter is more important.</p><div style="text-align:center;margin-bottom:1em;"><img src="`+M+'" width="70%" alt="Random search"><br> Fig: Random search </div><p>Experiment: Hyperparameters are often correlated.</p><div style="text-align:center;margin-bottom:1em;"><img src="'+R+'" width="70%" alt="Experiment"><br> Fig: Experiment </div><h3 id="when-resources-are-limited" tabindex="-1"><a class="header-anchor" href="#when-resources-are-limited"><span>When resources are limited</span></a></h3><p>Step 1. Check initial loss</p><p>Turn off weight decay, sanity check loss at initialization. e.g. log(C) for softmax with C classes.</p><p>Step 2. Overfit a small sample</p><p>Try to train 100% training accuracy on a small sample. 5-10 mini-batches. Turn off all regularization. fiddle with(随意摆弄) architecture, learning rate, weight initialization, etc.</p><p>Loss not going down ? LR too low, bad initialization Loss explodes to Inf or NaN ? LR too low, bad initialization.</p><p>Step 3. Find LR that makes loss go down</p><p>Use the architecture from the previous step, use all training data, turn on small weight decay, find a learning rate that makes the loss drop quickly within 100 iterations.</p><p>Good learning rates to try: 1e-1, 1e-2, 1e-3, 1e-4.</p><p>Step 4. Coarse grid, train for 1~5 epochs</p><p>Choose a few values of learning rate and weight decay around what worked from Step 3, train a few models for 1~5 epochs.</p><p>Good weight decays to try: 1e-5, 1e-4, 0.</p><p>Step 5. Refine grid, train longer</p><p>Step 6. Look at learning curves, fine-tune</p><p>Losses may be noisy, use scatter plot to see the trend.</p><h3 id="examples" tabindex="-1"><a class="header-anchor" href="#examples"><span>Examples</span></a></h3><div style="display:flex;justify-content:center;margin-bottom:1em;"><div style="text-align:center;margin-right:1em;"><img src="'+S+'" width="100%" alt="Learning curves"><br><img src="'+U+'" width="100%" alt="Learning curves"><br><img src="'+C+'" width="100%" alt="Learning curves"><br> Fig: Learning curves </div><div style="text-align:center;"><img src="'+N+'" width="100%"><br><img src="'+A+'" width="100%"><br><img src="'+F+'" width="100%"><br> Fig: Learning curves </div></div><p>Step 7. GOTO Step 5</p><p>Tuning is like DJing. You need to keep adjusting the knobs until you get the right sound 🎶</p><h3 id="tensorboard" tabindex="-1"><a class="header-anchor" href="#tensorboard"><span>TensorBoard</span></a></h3><p>TensorBoard is a visualization tool that comes with TensorFlow. It allows you to visualize your training process.</p><h3 id="track-ratio-of-weight-update-weight-magnitude" tabindex="-1"><a class="header-anchor" href="#track-ratio-of-weight-update-weight-magnitude"><span>Track ratio of weight update / weight magnitude</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="'+W+'" width="70%" alt="Track ratio of weight update / weight magnitude"><br> Fig: Track ratio of weight update / weight magnitude </div><h2 id="after-training" tabindex="-1"><a class="header-anchor" href="#after-training"><span>After training</span></a></h2><p>Model ensembles, transfer learning</p><h3 id="model-ensembles" tabindex="-1"><a class="header-anchor" href="#model-ensembles"><span>Model ensembles</span></a></h3><ol><li>Train multiple independent models</li><li>At test time, average their predictions (Take average of predicted probability distributions, then choose argmax)</li></ol><p>~ Enjoy 2% extra performance</p><p>Tips and tricks:</p><ul><li>Saving multiple checkpoints during training may also be a method of ensembling.</li><li>Keeps tracking the running average of the weights during training.</li><li>Periodic learning rate decay</li></ul><h3 id="transfer-learning" tabindex="-1"><a class="header-anchor" href="#transfer-learning"><span>Transfer learning</span></a></h3><p><s>&quot;You need a lot of data if want to train/use CNNs&quot;</s></p><h4 id="transfer-learning-with-cnns" tabindex="-1"><a class="header-anchor" href="#transfer-learning-with-cnns"><span>Transfer Learning with CNNs</span></a></h4><ol><li>Train on ImageNet</li><li>Remove the last FC</li><li>Use CNN as a feature extractor, freeze the weights of the CNN</li></ol><div style="text-align:center;margin-bottom:1em;"><img src="'+E+'" width="70%" alt="Transfer Learning with CNNs"><br> Fig: Transfer Learning with CNNs </div><ol start="4"><li>Bigger dataset: Fine-Tuning. Continue training the entire model for a new task.</li></ol><p>Some tricks:</p><ul><li>Train with feature extraction first before fine-tuning.</li><li>Lower the learning rate: use ~1/10 of LR used in original training.</li><li>Sometimes freeze lower layers to save computation.</li></ul><h4 id="architecture-matters" tabindex="-1"><a class="header-anchor" href="#architecture-matters"><span>Architecture Matters!</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="'+D+'" width="70%" alt="Transfer Learning with CNNs: Architecture Matters!"><br> Fig: Transfer Learning with CNNs </div><h4 id="transfer-learning-is-pervasive" tabindex="-1"><a class="header-anchor" href="#transfer-learning-is-pervasive"><span>Transfer learning is pervasive!</span></a></h4><p>It&#39;s the norm, not the exception</p><div style="text-align:center;margin-bottom:1em;"><img src="'+P+'" width="70%" alt="Transfer learning is pervasive"><br> Fig: Transfer learning is pervasive </div><div style="text-align:center;margin-bottom:1em;"><img src="'+q+'" width="70%" alt="Transfer learning is pervasive"><br> Fig: Transfer learning is pervasive </div><p>Pretraining - Transfer learning - Fine-tuning has become the norm.</p><h4 id="some-very-recent-results-have-questioned-it" tabindex="-1"><a class="header-anchor" href="#some-very-recent-results-have-questioned-it"><span>Some very recent results have questioned it</span></a></h4><p>Is this really critical?</p><p>Training from scratch can work as well as pretraining on ImageNet! ... If you train 3x as long</p><p>See <a href="https://arxiv.org/abs/1811.08883" target="_blank" rel="noopener noreferrer">Rethinking ImageNet Pre-training</a></p><p>Lots of work left to be done...</p><h3 id="large-batch-training" tabindex="-1"><a class="header-anchor" href="#large-batch-training"><span>Large batch training</span></a></h3><p>Scale the learning rate</p><div style="text-align:center;margin-bottom:1em;"><img src="'+I+'" width="70%" alt="Large batch training"><br> Fig: Large batch training </div><p>Learning Rate Warmup</p><p>Very large learning rate at the beginning may cause the model to diverge; linearly increasing learning rate from 0 over the first ~5000 iterations can prevent this.</p><p>Other Concerns:</p><p>Be careful with weight decay, batch normalization, and data shuffling.</p><p>For batch normalization, only normalize within a GPU.</p><p><a href="https://arxiv.org/abs/1706.02677" target="_blank" rel="noopener noreferrer">Training ImageNet in 1 hour</a></p><p>batch size = 8192, 256 GPUs</p><p>... and now we achieved several minutes to train ImageNet</p><h2 id="notice-on-usage-and-attribution" tabindex="-1"><a class="header-anchor" href="#notice-on-usage-and-attribution"><span><strong>Notice on Usage and Attribution</strong></span></a></h2><p>This note is based on the <strong>University of Michigan&#39;s publicly available course EECS 498.008 / 598.008</strong> and is intended <strong>solely for personal learning and academic discussion</strong>, with no commercial use.</p><ul><li><strong>Nature of the Notes:</strong> These notes include extensive references and citations from course materials to ensure clarity and completeness. However, they are presented as personal interpretations and summaries, not as substitutes for the original course content.</li><li><strong>Original Course Resources:</strong> Please refer to the official <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/" target="_blank" rel="noopener noreferrer"><strong>University of Michigan website</strong></a> for complete and accurate course materials.</li><li><strong>Third-Party Open Access Content:</strong> This note may reference Open Access (OA) papers or resources cited within the course materials. These materials are used under their original Open Access licenses (e.g., CC BY, CC BY-SA).</li><li><strong>Proper Attribution:</strong> Every referenced OA resource is appropriately cited, including the author, publication title, source link, and license type.</li><li><strong>Copyright Notice:</strong> All rights to third-party content remain with their respective authors or publishers.</li><li><strong>Content Removal:</strong> If you believe any content infringes on your copyright, please contact me, and I will promptly remove the content in question.</li></ul><p>Thanks to the <strong>University of Michigan</strong> and the contributors to the course for their openness and dedication to accessible education.</p>',171)])])}const j=t(G,[["render",O]]),H=JSON.parse('{"path":"/notes/um-cv/um-cv-10/","title":"10 & 11 Training Neural Networks","lang":"en-US","frontmatter":{"title":"10 & 11 Training Neural Networks","tags":["notes","computer-vision"],"createTime":"2024/12/26 17:45:38","permalink":"/notes/um-cv/um-cv-10/","outline":[2,4],"description":"Summary: Training neural networks, activation functions, data preprocessing, weight initialization, regularization, learning rate schedules, large batch training, hyperparameter...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"10 & 11 Training Neural Networks\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-21T17:44:17.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://saturntsen.github.io/notes/um-cv/um-cv-10/"}],["meta",{"property":"og:site_name","content":"SaturnTsen"}],["meta",{"property":"og:title","content":"10 & 11 Training Neural Networks"}],["meta",{"property":"og:description","content":"Summary: Training neural networks, activation functions, data preprocessing, weight initialization, regularization, learning rate schedules, large batch training, hyperparameter..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-02-21T17:44:17.000Z"}],["meta",{"property":"article:tag","content":"computer-vision"}],["meta",{"property":"article:tag","content":"notes"}],["meta",{"property":"article:modified_time","content":"2025-02-21T17:44:17.000Z"}]]},"readingTime":{"minutes":10.03,"words":3010},"git":{"createdTime":1735250901000,"updatedTime":1740159857000,"contributors":[{"name":"SaturnTsen","username":"SaturnTsen","email":"minger233@outlook.com","commits":6,"avatar":"https://avatars.githubusercontent.com/SaturnTsen?v=4","url":"https://github.com/SaturnTsen"}]},"autoDesc":true,"filePathRelative":"notes/UM-CV/UM-CV 10 Training Neural Networks.md","headers":[],"categoryList":[{"id":"4358b5","sort":10000,"name":"notes"},{"id":"31a781","sort":10004,"name":"UM-CV"}]}');export{j as comp,H as data};
