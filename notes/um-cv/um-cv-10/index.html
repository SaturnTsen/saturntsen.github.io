<!doctype html><html lang="en-US"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.24" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.161" /><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;const isDark = um === 'dark' || (um !== 'light' && sm);document.documentElement.dataset.theme = isDark ? 'dark' : 'light';})();</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"10 & 11 Training Neural Networks","image":[""],"dateModified":"2025-02-21T17:44:17.000Z","author":[]}</script><meta property="og:url" content="https://saturntsen.github.io/notes/um-cv/um-cv-10/"><meta property="og:site_name" content="SaturnTsen"><meta property="og:title" content="10 & 11 Training Neural Networks"><meta property="og:description" content="Summary: Training neural networks, activation functions, data preprocessing, weight initialization, regularization, learning rate schedules, large batch training, hyperparameter..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2025-02-21T17:44:17.000Z"><meta property="article:tag" content="computer-vision"><meta property="article:tag" content="notes"><meta property="article:modified_time" content="2025-02-21T17:44:17.000Z"><link rel="icon" href="/favicon.ico"><title>10 & 11 Training Neural Networks | SaturnTsen</title><meta name="description" content="Summary: Training neural networks, activation functions, data preprocessing, weight initialization, regularization, learning rate schedules, large batch training, hyperparameter..."><link rel="preload" href="/assets/style-CU0EtsvZ.css" as="style"><link rel="stylesheet" href="/assets/style-CU0EtsvZ.css"><link rel="modulepreload" href="/assets/app-BPmJL-vo.js"><link rel="modulepreload" href="/assets/index.html-Bf7LPeAD.js"><link rel="prefetch" href="/assets/index.html-BmG4TjtM.js" as="script"><link rel="prefetch" href="/assets/index.html-BQ1NaJuK.js" as="script"><link rel="prefetch" href="/assets/index.html-vVZNyw0-.js" as="script"><link rel="prefetch" href="/assets/index.html-ZDreb-Cr.js" as="script"><link rel="prefetch" href="/assets/index.html-DnwmR40J.js" as="script"><link rel="prefetch" href="/assets/index.html-DqKRuwzh.js" as="script"><link rel="prefetch" href="/assets/index.html-BnO6pfWi.js" as="script"><link rel="prefetch" href="/assets/index.html-DqrV2oWi.js" as="script"><link rel="prefetch" href="/assets/index.html-D0505yVi.js" as="script"><link rel="prefetch" href="/assets/index.html-D4PfOoAo.js" as="script"><link rel="prefetch" href="/assets/index.html-OZtXDBX-.js" as="script"><link rel="prefetch" href="/assets/index.html-DB8OfTW0.js" as="script"><link rel="prefetch" href="/assets/index.html-Cf43ABLN.js" as="script"><link rel="prefetch" href="/assets/index.html-Dwovho6D.js" as="script"><link rel="prefetch" href="/assets/index.html-83zXgr4a.js" as="script"><link rel="prefetch" href="/assets/index.html-BLtwLLZI.js" as="script"><link rel="prefetch" href="/assets/index.html-BUZheLiL.js" as="script"><link rel="prefetch" href="/assets/index.html-DnU5krJy.js" as="script"><link rel="prefetch" href="/assets/index.html-YZrr0wBf.js" as="script"><link rel="prefetch" href="/assets/index.html-bkdbp8wU.js" as="script"><link rel="prefetch" href="/assets/index.html-CWFawhma.js" as="script"><link rel="prefetch" href="/assets/index.html-DGAYbThF.js" as="script"><link rel="prefetch" href="/assets/index.html-C46ohglK.js" as="script"><link rel="prefetch" href="/assets/index.html-CSRYllLl.js" as="script"><link rel="prefetch" href="/assets/index.html-Cnmy6xJA.js" as="script"><link rel="prefetch" href="/assets/index.html-KI14JA6V.js" as="script"><link rel="prefetch" href="/assets/index.html-CFqE0fIB.js" as="script"><link rel="prefetch" href="/assets/index.html-Cp3T_Wdm.js" as="script"><link rel="prefetch" href="/assets/index.html-C5T_LiKK.js" as="script"><link rel="prefetch" href="/assets/index.html-RGl9jofY.js" as="script"><link rel="prefetch" href="/assets/index.html-C2o8eR02.js" as="script"><link rel="prefetch" href="/assets/index.html-C4s21r8r.js" as="script"><link rel="prefetch" href="/assets/index.html-BGMJZV3q.js" as="script"><link rel="prefetch" href="/assets/index.html-BnGAEkBd.js" as="script"><link rel="prefetch" href="/assets/index.html-f3JN0p-H.js" as="script"><link rel="prefetch" href="/assets/index.html-D3JrJcoe.js" as="script"><link rel="prefetch" href="/assets/index.html-DZjtQU4g.js" as="script"><link rel="prefetch" href="/assets/index.html-DPDml381.js" as="script"><link rel="prefetch" href="/assets/index.html-D-DIxWNy.js" as="script"><link rel="prefetch" href="/assets/index.html-M3FNVy5f.js" as="script"><link rel="prefetch" href="/assets/index.html-BxBVeWuK.js" as="script"><link rel="prefetch" href="/assets/index.html-pBhdW37w.js" as="script"><link rel="prefetch" href="/assets/index.html-CMGEEupf.js" as="script"><link rel="prefetch" href="/assets/index.html-XKTeTmUn.js" as="script"><link rel="prefetch" href="/assets/index.html-Nwfmu88j.js" as="script"><link rel="prefetch" href="/assets/index.html-DzKok2JV.js" as="script"><link rel="prefetch" href="/assets/index.html-CzQ1v4M_.js" as="script"><link rel="prefetch" href="/assets/index.html-b5bM2Mm0.js" as="script"><link rel="prefetch" href="/assets/index.html-4FD4vMR_.js" as="script"><link rel="prefetch" href="/assets/index.html-CELX4Gv6.js" as="script"><link rel="prefetch" href="/assets/index.html-DAuv_XsY.js" as="script"><link rel="prefetch" href="/assets/index.html-D0HnbJld.js" as="script"><link rel="prefetch" href="/assets/index.html-cizS7tpp.js" as="script"><link rel="prefetch" href="/assets/index.html-Db9qbygp.js" as="script"><link rel="prefetch" href="/assets/index.html-BC33mLRG.js" as="script"><link rel="prefetch" href="/assets/index.html-Bgjlk9gZ.js" as="script"><link rel="prefetch" href="/assets/index.html-DNyhKRbF.js" as="script"><link rel="prefetch" href="/assets/index.html-MHkPnQ_o.js" as="script"><link rel="prefetch" href="/assets/index.html-gtR08_vy.js" as="script"><link rel="prefetch" href="/assets/index.html-CoqucIIR.js" as="script"><link rel="prefetch" href="/assets/index.html-B6BIthZU.js" as="script"><link rel="prefetch" href="/assets/index.html-BKoKQbyt.js" as="script"><link rel="prefetch" href="/assets/index.html-Bshhoiba.js" as="script"><link rel="prefetch" href="/assets/index.html-nu7-Thlm.js" as="script"><link rel="prefetch" href="/assets/index.html-B1_Iy3K7.js" as="script"><link rel="prefetch" href="/assets/index.html-C5N22UG4.js" as="script"><link rel="prefetch" href="/assets/index.html-C2esroDP.js" as="script"><link rel="prefetch" href="/assets/index.html-BwFyN5XX.js" as="script"><link rel="prefetch" href="/assets/index.html-Cr91GHno.js" as="script"><link rel="prefetch" href="/assets/index.html-D6YLzvpj.js" as="script"><link rel="prefetch" href="/assets/index.html-Ds6znEeN.js" as="script"><link rel="prefetch" href="/assets/index.html-CAJn5R2r.js" as="script"><link rel="prefetch" href="/assets/index.html-CfYOQ07j.js" as="script"><link rel="prefetch" href="/assets/index.html-BpdD7CET.js" as="script"><link rel="prefetch" href="/assets/404.html-BRfb-qcV.js" as="script"><link rel="prefetch" href="/assets/index.html-DN0XcQYf.js" as="script"><link rel="prefetch" href="/assets/index.html-BrfGEaVq.js" as="script"><link rel="prefetch" href="/assets/index.html-DPdADZ69.js" as="script"><link rel="prefetch" href="/assets/index.html-Cxk-aAuN.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-CKV1Bsxh.js" as="script"><link rel="prefetch" href="/assets/giscus-BACEvYzX.js" as="script"><link rel="prefetch" href="/assets/searchBox-default-BgWbHU8C.js" as="script"><link rel="prefetch" href="/assets/SearchBox-D0_iY6j8.js" as="script"></head><body><div id="app"><!--[--><!--[--><div class="theme-plume vp-layout" vp-container data-v-a218b6db><!--[--><!--[--><!--]--><!--[--><span tabindex="-1" data-v-7a21891b></span><a href="#VPContent" class="vp-skip-link visually-hidden" data-v-7a21891b> Skip to content </a><!--]--><!----><header class="vp-nav" data-v-a218b6db data-v-cfd3cabe><div class="vp-navbar" vp-navbar data-v-cfd3cabe data-v-f0d7e165><div class="wrapper" data-v-f0d7e165><div class="container" data-v-f0d7e165><div class="title" data-v-f0d7e165><div class="vp-navbar-title has-sidebar" data-v-f0d7e165 data-v-78317e40><a class="vp-link link no-icon title" href="/" data-v-78317e40><!--[--><!--[--><!--]--><!--[--><!--[--><!--[--><img class="vp-image dark logo" style="" src="/avatars/avatar.png" alt data-v-17210522><!--]--><!--[--><img class="vp-image light logo" style="" src="/avatars/avatar.png" alt data-v-17210522><!--]--><!--]--><!--]--><span data-v-78317e40>SaturnTsen</span><!--[--><!--]--><!--]--><!----></a></div></div><div class="content" data-v-f0d7e165><div class="content-body" data-v-f0d7e165><!--[--><!--]--><div class="vp-navbar-search search" data-v-f0d7e165><div class="search-wrapper" data-v-895d8b33><!----><div id="local-search" data-v-895d8b33><button type="button" class="mini-search mini-search-button" aria-label="Search" data-v-895d8b33><span class="mini-search-button-container"><span class="mini-search-search-icon vpi-mini-search" aria-label="search icon"></span><span class="mini-search-button-placeholder">Search</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><!--[--><!--]--><nav aria-labelledby="main-nav-aria-label" class="vp-navbar-menu menu" data-v-f0d7e165 data-v-dd33332d><span id="main-nav-aria-label" class="visually-hidden" data-v-dd33332d>Main Navigation</span><!--[--><!--[--><a class="vp-link link navbar-menu-link" href="/" tabindex="0" data-v-dd33332d data-v-caac0954><!--[--><!----><span data-v-caac0954>Home</span><!----><!--]--><!----></a><!--]--><!--[--><div class="vp-flyout vp-navbar-menu-group" data-v-dd33332d data-v-3dde86b7><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-3dde86b7><span class="text" data-v-3dde86b7><!----><!----><span data-v-3dde86b7>Notes</span><!----><span class="vpi-chevron-down text-icon" data-v-3dde86b7></span></span></button><div class="menu" data-v-3dde86b7><div class="vp-menu" data-v-3dde86b7 data-v-55c02a15><div class="items" data-v-55c02a15><!--[--><!--[--><div class="vp-menu-link" data-v-55c02a15 data-v-b7034f0e><a class="vp-link link" href="/notes/um-cv/" data-v-b7034f0e><!--[--><!----> CV-NNDL <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-55c02a15 data-v-b7034f0e><a class="vp-link link" href="/notes/leetcode/" data-v-b7034f0e><!--[--><!----> LeetCode <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-55c02a15 data-v-b7034f0e><a class="vp-link link" href="/notes/misc/" data-v-b7034f0e><!--[--><!----> Misc <!----><!--]--><!----></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!--[--><!--]--><!----><div class="vp-navbar-appearance appearance" data-v-f0d7e165 data-v-be794682><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-be794682 data-v-42684fe4 data-v-196dff46><span class="check" data-v-196dff46><span class="icon" data-v-196dff46><!--[--><span class="vpi-sun sun" data-v-42684fe4></span><span class="vpi-moon moon" data-v-42684fe4></span><!--]--></span></span></button></div><div class="vp-social-links vp-navbar-social-links social-links" data-v-f0d7e165 data-v-a8dae519 data-v-865113bf><!--[--><a class="vp-social-link no-icon" href="https://github.com/saturntsen" aria-label="github" target="_blank" rel="noopener" data-v-865113bf data-v-08f84eea><span class="vpi-social-github" /></a><!--]--></div><div class="vp-flyout vp-navbar-extra extra" data-v-f0d7e165 data-v-59aeafe8 data-v-3dde86b7><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-3dde86b7><span class="vpi-more-horizontal icon" data-v-3dde86b7></span></button><div class="menu" data-v-3dde86b7><div class="vp-menu" data-v-3dde86b7 data-v-55c02a15><!----><!--[--><!--[--><!----><div class="group" data-v-59aeafe8><div class="item appearance" data-v-59aeafe8><p class="label" data-v-59aeafe8>Appearance</p><div class="appearance-action" data-v-59aeafe8><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-59aeafe8 data-v-42684fe4 data-v-196dff46><span class="check" data-v-196dff46><span class="icon" data-v-196dff46><!--[--><span class="vpi-sun sun" data-v-42684fe4></span><span class="vpi-moon moon" data-v-42684fe4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-59aeafe8><div class="item social-links" data-v-59aeafe8><div class="vp-social-links social-links-list" data-v-59aeafe8 data-v-865113bf><!--[--><a class="vp-social-link no-icon" href="https://github.com/saturntsen" aria-label="github" target="_blank" rel="noopener" data-v-865113bf data-v-08f84eea><span class="vpi-social-github" /></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="vp-navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-f0d7e165 data-v-c65a6891><span class="container" data-v-c65a6891><span class="top" data-v-c65a6891></span><span class="middle" data-v-c65a6891></span><span class="bottom" data-v-c65a6891></span></span></button></div></div></div></div><div class="divider" data-v-f0d7e165><div class="divider-line" data-v-f0d7e165></div></div></div><!----></header><div class="vp-local-nav reached-top" data-v-a218b6db data-v-e2935d30><button class="menu" aria-expanded="false" aria-controls="SidebarNav" data-v-e2935d30><span class="vpi-align-left menu-icon" data-v-e2935d30></span><span class="menu-text" data-v-e2935d30>Menu</span></button><div class="vp-local-nav-outline-dropdown" style="--vp-vh:0px;" data-v-e2935d30 data-v-ff10071c><button data-v-ff10071c>Return to top</button><!----></div></div><aside class="vp-sidebar" vp-sidebar data-v-a218b6db data-v-4f040617><div class="curtain" data-v-4f040617></div><nav id="SidebarNav" class="nav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-4f040617><span id="sidebar-aria-label" class="visually-hidden" data-v-4f040617> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-80522f8e><section class="vp-sidebar-item sidebar-item level-0 has-active" data-v-80522f8e data-v-467b1cf7><div class="item" role="button" tabindex="0" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><h2 class="text" data-v-467b1cf7><span data-v-467b1cf7>UM-CV</span><!----></h2><!----></div><div data-v-467b1cf7 data-v-467b1cf7><div class="items" data-v-467b1cf7><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>README</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-1/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>1 Intro</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-2/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>2 Image Classification</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-3-4/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>3 & 4 Linear Classifiers and Optimization</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-5/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>5 & 6 Neural Networks and Back Propagation</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/backprop-calc-1/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>Two-layer MLP backprop</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/backprop-calc-2/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>Batch Normalization backprop</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-7/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>7 & 8 CNN and its design principles</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-9/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>9 Hardware, Software, PyTorch Modules</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-10/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>10 & 11 Training Neural Networks</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-12/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>12 & 13 Recurrent Neural Networks & Attention Mechanism</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-14/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>14 Visualizing and understanding CNNs</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-15/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>15 Object Detection</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-16/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>16 Object Semantic Segmentation</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-17/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>17 3D Vision</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-18/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>18 Videos</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-19/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>19 Autoregressive Models, Variational Autoencoders</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-20/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>20 Generative Adversarial Networks</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-21/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>21 Reinforcement Learning</span><!----></p><!--]--><!----></a><!----></div><!----></div><!--]--></div></div></section></div><!--]--><!--[--><!--]--></nav></aside><!--[--><div id="VPContent" vp-content class="vp-content has-sidebar" data-v-a218b6db data-v-6e8aa8db><div class="vp-doc-container has-sidebar has-aside" data-v-6e8aa8db data-v-380c246c><!--[--><!--]--><div class="container" data-v-380c246c><div class="aside" vp-outline data-v-380c246c><div class="aside-curtain" data-v-380c246c></div><div class="aside-container" data-v-380c246c><div class="aside-content" data-v-380c246c><div class="vp-doc-aside" data-v-380c246c data-v-36d29b94><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="vp-doc-aside-outline" role="navigation" data-v-36d29b94 data-v-924d914c><div class="content" data-v-924d914c><div class="outline-marker" data-v-924d914c></div><div id="doc-outline-aria-label" aria-level="2" class="outline-title" role="heading" data-v-924d914c><span data-v-924d914c>On this page</span><span class="vpi-print icon" data-v-924d914c></span></div><ul class="root" data-v-924d914c data-v-fc78de7f><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-36d29b94></div><!--[--><!--]--></div></div></div></div><div class="content" data-v-380c246c><div class="content-container" data-v-380c246c><!--[--><!--]--><main class="main" data-v-380c246c><nav class="vp-breadcrumb" data-v-380c246c data-v-389ad6f0><ol vocab="https://schema.org/" typeof="BreadcrumbList" data-v-389ad6f0><!--[--><li property="itemListElement" typeof="ListItem" data-v-389ad6f0><a class="vp-link link breadcrumb" href="/" property="item" typeof="WebPage" data-v-389ad6f0><!--[-->Home<!--]--><!----></a><span class="vpi-chevron-right" data-v-389ad6f0></span><meta property="name" content="Home" data-v-389ad6f0><meta property="position" content="1" data-v-389ad6f0></li><li property="itemListElement" typeof="ListItem" data-v-389ad6f0><span class="vp-link breadcrumb" property="item" typeof="WebPage" data-v-389ad6f0><!--[-->UM-CV<!--]--><!----></span><span class="vpi-chevron-right" data-v-389ad6f0></span><meta property="name" content="UM-CV" data-v-389ad6f0><meta property="position" content="2" data-v-389ad6f0></li><li property="itemListElement" typeof="ListItem" data-v-389ad6f0><a class="vp-link link breadcrumb current" href="/notes/um-cv/um-cv-10/" property="item" typeof="WebPage" data-v-389ad6f0><!--[-->10 &amp; 11 Training Neural Networks<!--]--><!----></a><!----><meta property="name" content="10 &amp; 11 Training Neural Networks" data-v-389ad6f0><meta property="position" content="3" data-v-389ad6f0></li><!--]--></ol></nav><!--[--><!--]--><!--[--><h1 class="vp-doc-title page-title" data-v-8dd3566a><!----> 10 &amp; 11 Training Neural Networks <!----></h1><div class="vp-doc-meta" data-v-8dd3566a><!--[--><!--]--><p class="reading-time" data-v-8dd3566a><span class="vpi-books icon" data-v-8dd3566a></span><span data-v-8dd3566a>About 3010 words</span><span data-v-8dd3566a>About 10 min</span></p><p data-v-8dd3566a><span class="vpi-tag icon" data-v-8dd3566a></span><!--[--><span class="vp-link tag vp-tag-9wic" data-v-8dd3566a><!--[-->notes<!--]--><!----></span><span class="vp-link tag vp-tag-akhq" data-v-8dd3566a><!--[-->computer-vision<!--]--><!----></span><!--]--></p><!--[--><!--]--><p class="create-time" data-v-8dd3566a><span class="vpi-clock icon" data-v-8dd3566a></span><span data-v-8dd3566a>2024-12-26</span></p></div><!--]--><!--[--><!--]--><div class="_notes_um-cv_um-cv-10_ external-link-icon-enabled vp-doc plume-content" vp-content data-v-380c246c><!--[--><!--]--><div data-v-380c246c><p>Summary: Training neural networks, activation functions, data preprocessing, weight initialization, regularization, learning rate schedules, large batch training, hyperparameter tuning, model ensembles, transfer learning.</p><p>@Credits: <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/" target="_blank" rel="noopener noreferrer">EECS 498.007</a> | Video Lecture: <a href="https://www.youtube.com/watch?v=dJYGatp4SvA&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r" target="_blank" rel="noopener noreferrer">UM-CV</a></p><p>Personal work for the assignments of the course: <a href="https://github.com/SaturnTsen/EECS-498-007/" target="_blank" rel="noopener noreferrer">github repo</a>.</p><p><strong>Notice on Usage and Attribution</strong></p><p>These are personal class notes based on the University of Michigan EECS 498.008 / 598.008 course. They are intended solely for personal learning and academic discussion, with no commercial use.</p><p>For detailed information, please refer to the <strong><a href="#notice-on-usage-and-attribution">complete notice at the end of this document</a></strong></p><h2 id="one-time-setup" tabindex="-1"><a class="header-anchor" href="#one-time-setup"><span>One-time setup</span></a></h2><p>Activation functions, data preprocessing, weight initialization, regularization</p><h3 id="activation-functions" tabindex="-1"><a class="header-anchor" href="#activation-functions"><span>Activation functions</span></a></h3><p>Activation functions adds critical linearity for neural networks</p><h4 id="sigmoid" tabindex="-1"><a class="header-anchor" href="#sigmoid"><span>Sigmoid</span></a></h4><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\sigma(x) = \frac{1}{1 + e^{-x}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0908em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6973em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><ul><li>Squashes numbers to range [0,1]</li><li>Historically popular since they have nice interpretation as a starting &quot;firing rate&quot; of a neuron</li></ul><p>3 problems:</p><ol><li>Saturated neurons kill the gradients. (The most problematic aspect)</li><li>Sigmoid outputs are not zero-centered. Suppose a multi-layer network, then the inputs of all the layers are always positive Also the gradient of this function is always positive. All the gradients for the weights will have the same sign, and gradients will always push the weights into the same direction. This becomes less of a problem when using mini-batches.</li></ol><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-1.png" width="80%" alt="Gradient update directions"><br> Fig: Gradient update directions </div><ol start="3"><li>exp() is a bit compute expensive: transcendental function. For GPUs, this is not a big deal, the copying takes more time than the computation.</li></ol><h4 id="tanh" tabindex="-1"><a class="header-anchor" href="#tanh"><span>Tanh</span></a></h4><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac><mo>=</mo><mn>2</mn><mi>σ</mi><mo stretchy="false">(</mo><mn>2</mn><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">tanh</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2177em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.4483em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5904em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6973em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7713em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord">2</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span></p><ul><li>Squashes numbers to range [-1,1]</li><li>zero centered</li><li>still kills gradients when saturated 😦</li></ul><h4 id="relu" tabindex="-1"><a class="header-anchor" href="#relu"><span>ReLU</span></a></h4><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>ReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{ReLU}(x) = \max(0, x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">ReLU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><ul><li>Does not saturate in the positive region</li><li>Computationally efficient</li><li>Converges much faster than sigmoid/tanh in practice (e.g. 6x)</li></ul><p>Problems:</p><ul><li>Not zero-centered output</li><li>Dying ReLU problem: neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this case, the gradient flowing through a ReLU neuron is always 0 because the gradient of max(0, x) is 0 if x &lt; 0.</li></ul><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-2.png" width="80%" alt="Dying ReLU problem"><br> Fig: Dying ReLU problem </div><p>dead ReLU will never activate -&gt; never update its weights</p><h4 id="leaky-relu" tabindex="-1"><a class="header-anchor" href="#leaky-relu"><span>Leaky ReLU</span></a></h4><p>Sometimes initialize ReLU neurons with a slightly positive slope in order to mitigate the dying ReLU problem.</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>LeakyReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0.01</mn><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{LeakyReLU}(x) = \max(0.01x, x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">LeakyReLU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0.01</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><ul><li>Does not saturate</li><li>Computationally efficient</li><li>Converges much faster than sigmoid/tanh in practice! (e.g. 6x)</li><li>will not &quot;die&quot;</li></ul><h4 id="parametric-relu-prelu" tabindex="-1"><a class="header-anchor" href="#parametric-relu-prelu"><span>Parametric ReLU (PReLU)</span></a></h4><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>PReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>α</mi><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{PReLU}(x) = \max(\alpha x, x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">PReLU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathnormal">αx</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is learned and backpropagated through.</p><h4 id="exponential-linear-units-elu" tabindex="-1"><a class="header-anchor" href="#exponential-linear-units-elu"><span>Exponential Linear Units (ELU)</span></a></h4><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>ELU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>x</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mi>x</mi><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>α</mi><mo stretchy="false">(</mo><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mi>x</mi><mo>≤</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex"> \text{ELU}(x) = \begin{cases} x &amp; \text{if } x &gt; 0 \\ \alpha(e^x - 1) &amp; \text{if } x \leq 0 \end{cases} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">ELU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3em;vertical-align:-1.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">if </span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">if </span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><ul><li>default <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></li><li>All benefits of ReLU</li><li>Closer to zero mean outputs</li><li>Negative saturation regime compared with ReLU</li></ul><p>Problem:</p><ul><li>Computationally more expensive</li></ul><h4 id="selu" tabindex="-1"><a class="header-anchor" href="#selu"><span>SELU</span></a></h4><p>A rescaled version of ELU</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>SELU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>λ</mi><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>x</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mi>x</mi><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>α</mi><mo stretchy="false">(</mo><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><mi>x</mi><mo>≤</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\text{SELU}(x) = \lambda \begin{cases} x &amp; \text{if } x &gt; 0 \\ \alpha(e^x - 1) &amp; \text{if } x \leq 0 \end{cases} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">SELU</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3em;vertical-align:-1.25em;"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">if </span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">if </span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1.6732632423543772848170429916717</mn></mrow><annotation encoding="application/x-tex">\alpha = 1.6732632423543772848170429916717</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1.6732632423543772848170429916717</span></span></span></span></p><p>and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>1.0507009873554804934193349852946</mn></mrow><annotation encoding="application/x-tex">\lambda = 1.0507009873554804934193349852946</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1.0507009873554804934193349852946</span></span></span></span></p><ul><li>Scaled version of ELU that works better for deep networks</li><li>Self-normalizing property: SELU activations preserve mean and variance of inputs. Can train deep SELU neural networks without normalization layers.</li></ul><h4 id="comparison-of-activation-functions" tabindex="-1"><a class="header-anchor" href="#comparison-of-activation-functions"><span>Comparison of activation functions</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-3.png" width="80%" alt="Comparison of activation functions"><br> Fig: Comparison of activation functions </div><p>Summary:</p><ul><li>Don&#39;t think too hard. Just use ReLU.</li><li>Try out Leaky ReLU/ELU/SELU/GELU if you need to squeeze that last 0.1%</li><li>Don&#39;t use sigmoid or tanh. The models will learn very slowly.</li></ul><h3 id="data-preprocessing" tabindex="-1"><a class="header-anchor" href="#data-preprocessing"><span>Data preprocessing</span></a></h3><p>We want to normalize the inputs to have zero mean and unit variance. This helps the model learn faster and prevents the gradients from going out of control.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-4.png" width="80%" alt="Data preprocessing"><br> Fig: Data preprocessing </div><p>In practice, you may also see PCA and Whitening applied to the data.</p><p>Rotate the data so that the principal components are aligned with the axes. This is called PCA whitening. For image data this is not so common.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-5.png" width="80%" alt="PCA whitening"><br> Fig: PCA whitening </div><p>Why do we need to normalize the data?</p><p>Classification loss would be very sensitive to changes in weight matrix; hard to optimize.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-6.png" width="80%" alt="Classification"><br> Fig: Classification</div><h4 id="real-examples" tabindex="-1"><a class="header-anchor" href="#real-examples"><span>Real examples</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-7.png" width="60%" alt="Real examples"><br> Fig: Real examples </div><h3 id="weight-initialization" tabindex="-1"><a class="header-anchor" href="#weight-initialization"><span>Weight initialization</span></a></h3><p>Zero initialization is bad because all neurons will have the same gradient and will update in the same way.</p><p>Weight Initialization: small random numbers. Fairly ok with shallow networks, but problems with deeper networks.</p><p>This is to ensure that the gradient behaves nicely at the beginning of training.</p><p>Activation Statistics: Histogram of each layers</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-8.png" width="60%" alt="Statistics of a sample x"><br> Fig: Statistics of a sample x </div><p>Problem: This initialization may be too small for deep networks.</p><p>If we initialize the weights too big, all neurons will be saturated and the gradients will be zero.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-9.png" width="70%" alt="Statistics of a sample x"><br> Fig: Statistics of a sample x when weights are too big </div><h4 id="xavier-initialization" tabindex="-1"><a class="header-anchor" href="#xavier-initialization"><span>Xavier initialization</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-10.png" width="70%" alt="Xavier initialization"><br> Fig: Xavier initialization </div><p>Derivation: Variance of output = Variance of inputs</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-11.png" width="70%" alt="Derivation"><br> Fig: Derivation </div><p>We are only taking about linear layers. For ReLU, this method will collapse to zero again, no learning 😦</p><h4 id="kaiming-msra-initialization" tabindex="-1"><a class="header-anchor" href="#kaiming-msra-initialization"><span>Kaiming/MSRA initialization</span></a></h4><p>Multiply Xavier by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mn>2</mn></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1328em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9072em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord">2</span></span></span><span style="top:-2.8672em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1328em;"><span></span></span></span></span></span></span></span></span> for ReLU activation functions.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-12.png" width="70%" alt="Kaiming/MSRA initialization"><br> Fig: Kaiming/MSRA initialization </div><p>This keeps the variance of the output the same as the variance of the input.</p><p>This is sufficient to got VGG to train from scratch.</p><h4 id="residual-networks" tabindex="-1"><a class="header-anchor" href="#residual-networks"><span>Residual Networks</span></a></h4><p>MSRA is not that useful for residual networks.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-13.png" width="70%" alt="Residual Networks"><br> Fig: Residual Networks </div><h3 id="regularization" tabindex="-1"><a class="header-anchor" href="#regularization"><span>Regularization</span></a></h3><p>Add term to the loss.</p><h3 id="weight-decay" tabindex="-1"><a class="header-anchor" href="#weight-decay"><span>Weight decay</span></a></h3><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo>=</mo><mtext>data loss</mtext><mo>+</mo><mi>λ</mi><mi>R</mi><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L = \text{data loss} + \lambda R(W) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord">data loss</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">λ</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span></span></span></span></span></p><p>In common we use:</p><ul><li>L2 regularization: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><msubsup><mi>W</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">R(W) = \sum_i W_i^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1138em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span> (weight decay)</li><li>L1 regularization: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>W</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">R(W) = \sum_i |W_i|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span></li><li>Elastic net: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>W</mi><mo stretchy="false">)</mo><mo>=</mo><mi>α</mi><msub><mo>∑</mo><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>W</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mo>+</mo><mi>β</mi><msub><mo>∑</mo><mi>i</mi></msub><msubsup><mi>W</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">R(W) = \alpha \sum_i |W_i| + \beta \sum_i W_i^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1138em;vertical-align:-0.2997em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span> (L1 + L2)</li></ul><h3 id="dropout" tabindex="-1"><a class="header-anchor" href="#dropout"><span>Dropout</span></a></h3><p>Randomly set some neurons to zero during forward and backward pass.</p><p>We want to prevent the network from relying too much on any one neuron. Prevents co-adaptation of neurons.</p><p>Another interpretation: Ensemble of networks. Dropout is like training an ensemble of networks and averaging their predictions.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-14.png" width="70%" alt="Dropout"><br> Fig: Dropout </div><p>Problem: Test time. We want to use all the neurons.</p><p>We average out the randomness at test-time, but this integral seems hard... So we need to approximate the integral.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-15.png" width="70%" alt="Dropout at test time"><br> Fig: Dropout at test time </div><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-16.png" width="70%" alt="Dropout at test time"><br> Fig: Dropout at test time </div><p>At test time, we multiply the weights by the dropout probability.</p><p>At test time all neurons are active always. =&gt; We must scale the activations so that for each neuron, the expected output is the same as the expected output at training time.</p><p><strong>Drop in forward pass, scale in backward pass.</strong></p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-17.png" width="70%" alt="Dropout at test time"><br> Fig: Inverted dropout </div><p>Dropout architecture is not common in modern architectures.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-18.png" width="70%" alt="Dropout at test time"><br> Fig: Dropout at test time </div><h3 id="batch-normalization" tabindex="-1"><a class="header-anchor" href="#batch-normalization"><span>Batch normalization</span></a></h3><p>We have already learned this in the previous lecture.</p><p>For ResNet and later, often L2 and batch normalization are the only regularizers!</p><h3 id="data-augmentation" tabindex="-1"><a class="header-anchor" href="#data-augmentation"><span>Data Augmentation</span></a></h3><ul><li>Random crops</li><li>Random flips</li><li>Random scales</li><li>Color jittering</li></ul><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/10-19.png" width="70%" alt="Data Augmentation"><br> Fig: Data Augmentation </div><p>Get creative for you problem!</p><p>Random mix/combinations of:</p><ul><li>Translation</li><li>Rotation</li><li>Stretching</li><li>Shearing</li><li>lens distortions,... (go crazy)</li></ul><h3 id="other-regularizers" tabindex="-1"><a class="header-anchor" href="#other-regularizers"><span>Other regularizers</span></a></h3><h4 id="dropconnect" tabindex="-1"><a class="header-anchor" href="#dropconnect"><span>DropConnect</span></a></h4><p>Training: Instead of dropping out neurons, drop out weights. Testing: Use all the connections.</p><h4 id="fractional-max-pooling" tabindex="-1"><a class="header-anchor" href="#fractional-max-pooling"><span>Fractional Max Pooling</span></a></h4><p>Training: Randomize the size of the pooling region. Testing: Average predictions over different samples.</p><h4 id="stochastic-depth" tabindex="-1"><a class="header-anchor" href="#stochastic-depth"><span>Stochastic Depth</span></a></h4><p>Training: Skip some residual blocks in ResNet.</p><p>Testing: Use the whole network.</p><h4 id="stochastic-depth-1" tabindex="-1"><a class="header-anchor" href="#stochastic-depth-1"><span>Stochastic Depth</span></a></h4><p>Training: Set random images regions to 0 Testing: Use the whole image</p><h4 id="mixup" tabindex="-1"><a class="header-anchor" href="#mixup"><span>Mixup</span></a></h4><p>Training: Train on random blends of images Testing: Use original images</p><h2 id="training-dynamics" tabindex="-1"><a class="header-anchor" href="#training-dynamics"><span>Training dynamics</span></a></h2><p>learning rate schedules, large batch training, hyperparameter tuning</p><h3 id="learning-rate-schedules" tabindex="-1"><a class="header-anchor" href="#learning-rate-schedules"><span>Learning rate schedules</span></a></h3><p>Starting with hight learning rate and lower it over time.</p><p>Common schedules:</p><ul><li>Step decay: lower the learning rate by a factor every few fixed epochs.</li></ul><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/11-1.png" width="70%" alt="Step decay"><br> Fig: Step decay </div> - Cosine: lower the learning rate according to a cosine schedule. This has less hyperparameters than step decay. Often used for CV. <div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/11-2.png" width="70%" alt="Cosine"><br> Fig: Cosine </div><ul><li>Linear: linearly decrease the learning rate over time. Often used for NLP.</li><li>Inverse square root: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>α</mi><msqrt><mi>t</mi></msqrt></mfrac></mrow><annotation encoding="application/x-tex">\frac{\alpha}{\sqrt{t}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2334em;vertical-align:-0.538em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6954em;"><span style="top:-2.5613em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8982em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-2.8582em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1418em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.0037em;">α</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> is the initial learning rate and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> is the iteration number.</li><li>Constant: keep the learning rate constant. This works already quite well. A constant schedule is often used for fine-tuning. Adam is already adaptive.</li></ul><p>Tip: <code>torch.zero_grad()</code> is important. If you don&#39;t zero the gradients, the gradients will accumulate.</p><p>Usually for Classification problem, the loss will not explode after a long time, but this is possible for other problems, e.g. Reinforcement Learning. (General experience)</p><h3 id="early-stopping" tabindex="-1"><a class="header-anchor" href="#early-stopping"><span>Early stopping</span></a></h3><p>Stop training the model when accuracy on the validation set decreases, or train for a long time, but always keep track of the model snapshot that worked best on val. Always a good idea to do this!</p><h3 id="choosing-hyperparameters" tabindex="-1"><a class="header-anchor" href="#choosing-hyperparameters"><span>Choosing hyperparameters</span></a></h3><h4 id="grid-search" tabindex="-1"><a class="header-anchor" href="#grid-search"><span>Grid search</span></a></h4><p>Choose several values for each hyperparameter often chosen log-linearly. Evaluate all possible choices on this hyperparameter grid.</p><h4 id="random-search" tabindex="-1"><a class="header-anchor" href="#random-search"><span>Random search</span></a></h4><p>Choose several intervals for each hyperparameter often chosen log-linearly.</p><p>Run many different trails randomly.</p><p>This allow us to sample more hyperparameters along one dimension. This is useful when one hyperparameter is more important.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/11-3.png" width="70%" alt="Random search"><br> Fig: Random search </div><p>Experiment: Hyperparameters are often correlated.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/11-4.png" width="70%" alt="Experiment"><br> Fig: Experiment </div><h3 id="when-resources-are-limited" tabindex="-1"><a class="header-anchor" href="#when-resources-are-limited"><span>When resources are limited</span></a></h3><p>Step 1. Check initial loss</p><p>Turn off weight decay, sanity check loss at initialization. e.g. log(C) for softmax with C classes.</p><p>Step 2. Overfit a small sample</p><p>Try to train 100% training accuracy on a small sample. 5-10 mini-batches. Turn off all regularization. fiddle with(随意摆弄) architecture, learning rate, weight initialization, etc.</p><p>Loss not going down ? LR too low, bad initialization Loss explodes to Inf or NaN ? LR too low, bad initialization.</p><p>Step 3. Find LR that makes loss go down</p><p>Use the architecture from the previous step, use all training data, turn on small weight decay, find a learning rate that makes the loss drop quickly within 100 iterations.</p><p>Good learning rates to try: 1e-1, 1e-2, 1e-3, 1e-4.</p><p>Step 4. Coarse grid, train for 1~5 epochs</p><p>Choose a few values of learning rate and weight decay around what worked from Step 3, train a few models for 1~5 epochs.</p><p>Good weight decays to try: 1e-5, 1e-4, 0.</p><p>Step 5. Refine grid, train longer</p><p>Step 6. Look at learning curves, fine-tune</p><p>Losses may be noisy, use scatter plot to see the trend.</p><h3 id="examples" tabindex="-1"><a class="header-anchor" href="#examples"><span>Examples</span></a></h3><div style="display:flex;justify-content:center;margin-bottom:1em;"><div style="text-align:center;margin-right:1em;"><img src="/images/um-cv/11-5.png" width="100%" alt="Learning curves"><br><img src="/images/um-cv/11-6.png" width="100%" alt="Learning curves"><br><img src="/images/um-cv/11-7.png" width="100%" alt="Learning curves"><br> Fig: Learning curves </div><div style="text-align:center;"><img src="/images/um-cv/11-8.png" width="100%"><br><img src="/images/um-cv/11-9.png" width="100%"><br><img src="/images/um-cv/11-10.png" width="100%"><br> Fig: Learning curves </div></div><p>Step 7. GOTO Step 5</p><p>Tuning is like DJing. You need to keep adjusting the knobs until you get the right sound 🎶</p><h3 id="tensorboard" tabindex="-1"><a class="header-anchor" href="#tensorboard"><span>TensorBoard</span></a></h3><p>TensorBoard is a visualization tool that comes with TensorFlow. It allows you to visualize your training process.</p><h3 id="track-ratio-of-weight-update-weight-magnitude" tabindex="-1"><a class="header-anchor" href="#track-ratio-of-weight-update-weight-magnitude"><span>Track ratio of weight update / weight magnitude</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/11-11.png" width="70%" alt="Track ratio of weight update / weight magnitude"><br> Fig: Track ratio of weight update / weight magnitude </div><h2 id="after-training" tabindex="-1"><a class="header-anchor" href="#after-training"><span>After training</span></a></h2><p>Model ensembles, transfer learning</p><h3 id="model-ensembles" tabindex="-1"><a class="header-anchor" href="#model-ensembles"><span>Model ensembles</span></a></h3><ol><li>Train multiple independent models</li><li>At test time, average their predictions (Take average of predicted probability distributions, then choose argmax)</li></ol><p>~ Enjoy 2% extra performance</p><p>Tips and tricks:</p><ul><li>Saving multiple checkpoints during training may also be a method of ensembling.</li><li>Keeps tracking the running average of the weights during training.</li><li>Periodic learning rate decay</li></ul><h3 id="transfer-learning" tabindex="-1"><a class="header-anchor" href="#transfer-learning"><span>Transfer learning</span></a></h3><p><s>&quot;You need a lot of data if want to train/use CNNs&quot;</s></p><h4 id="transfer-learning-with-cnns" tabindex="-1"><a class="header-anchor" href="#transfer-learning-with-cnns"><span>Transfer Learning with CNNs</span></a></h4><ol><li>Train on ImageNet</li><li>Remove the last FC</li><li>Use CNN as a feature extractor, freeze the weights of the CNN</li></ol><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/11-12.png" width="70%" alt="Transfer Learning with CNNs"><br> Fig: Transfer Learning with CNNs </div><ol start="4"><li>Bigger dataset: Fine-Tuning. Continue training the entire model for a new task.</li></ol><p>Some tricks:</p><ul><li>Train with feature extraction first before fine-tuning.</li><li>Lower the learning rate: use ~1/10 of LR used in original training.</li><li>Sometimes freeze lower layers to save computation.</li></ul><h4 id="architecture-matters" tabindex="-1"><a class="header-anchor" href="#architecture-matters"><span>Architecture Matters!</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/11-13.png" width="70%" alt="Transfer Learning with CNNs: Architecture Matters!"><br> Fig: Transfer Learning with CNNs </div><h4 id="transfer-learning-is-pervasive" tabindex="-1"><a class="header-anchor" href="#transfer-learning-is-pervasive"><span>Transfer learning is pervasive!</span></a></h4><p>It&#39;s the norm, not the exception</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/11-14.png" width="70%" alt="Transfer learning is pervasive"><br> Fig: Transfer learning is pervasive </div><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/11-15.png" width="70%" alt="Transfer learning is pervasive"><br> Fig: Transfer learning is pervasive </div><p>Pretraining - Transfer learning - Fine-tuning has become the norm.</p><h4 id="some-very-recent-results-have-questioned-it" tabindex="-1"><a class="header-anchor" href="#some-very-recent-results-have-questioned-it"><span>Some very recent results have questioned it</span></a></h4><p>Is this really critical?</p><p>Training from scratch can work as well as pretraining on ImageNet! ... If you train 3x as long</p><p>See <a href="https://arxiv.org/abs/1811.08883" target="_blank" rel="noopener noreferrer">Rethinking ImageNet Pre-training</a></p><p>Lots of work left to be done...</p><h3 id="large-batch-training" tabindex="-1"><a class="header-anchor" href="#large-batch-training"><span>Large batch training</span></a></h3><p>Scale the learning rate</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/11-16.png" width="70%" alt="Large batch training"><br> Fig: Large batch training </div><p>Learning Rate Warmup</p><p>Very large learning rate at the beginning may cause the model to diverge; linearly increasing learning rate from 0 over the first ~5000 iterations can prevent this.</p><p>Other Concerns:</p><p>Be careful with weight decay, batch normalization, and data shuffling.</p><p>For batch normalization, only normalize within a GPU.</p><p><a href="https://arxiv.org/abs/1706.02677" target="_blank" rel="noopener noreferrer">Training ImageNet in 1 hour</a></p><p>batch size = 8192, 256 GPUs</p><p>... and now we achieved several minutes to train ImageNet</p><h2 id="notice-on-usage-and-attribution" tabindex="-1"><a class="header-anchor" href="#notice-on-usage-and-attribution"><span><strong>Notice on Usage and Attribution</strong></span></a></h2><p>This note is based on the <strong>University of Michigan&#39;s publicly available course EECS 498.008 / 598.008</strong> and is intended <strong>solely for personal learning and academic discussion</strong>, with no commercial use.</p><ul><li><strong>Nature of the Notes:</strong> These notes include extensive references and citations from course materials to ensure clarity and completeness. However, they are presented as personal interpretations and summaries, not as substitutes for the original course content.</li><li><strong>Original Course Resources:</strong> Please refer to the official <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/" target="_blank" rel="noopener noreferrer"><strong>University of Michigan website</strong></a> for complete and accurate course materials.</li><li><strong>Third-Party Open Access Content:</strong> This note may reference Open Access (OA) papers or resources cited within the course materials. These materials are used under their original Open Access licenses (e.g., CC BY, CC BY-SA).</li><li><strong>Proper Attribution:</strong> Every referenced OA resource is appropriately cited, including the author, publication title, source link, and license type.</li><li><strong>Copyright Notice:</strong> All rights to third-party content remain with their respective authors or publishers.</li><li><strong>Content Removal:</strong> If you believe any content infringes on your copyright, please contact me, and I will promptly remove the content in question.</li></ul><p>Thanks to the <strong>University of Michigan</strong> and the contributors to the course for their openness and dedication to accessible education.</p></div><!----><!----><!----></div></main><footer class="vp-doc-footer" data-v-380c246c data-v-7bfa3324><!--[--><!--]--><div class="edit-info" data-v-7bfa3324><div class="edit-link" data-v-7bfa3324><a class="vp-link link no-icon edit-link-button" href="https://github.com/SaturnTsen/saturntsen.github.io/edit/main/docs/notes/UM-CV/UM-CV 10 Training Neural Networks.md" target="_blank" rel="noreferrer" data-v-7bfa3324><!--[--><span class="vpi-square-pen edit-link-icon" aria-label="edit icon" data-v-7bfa3324></span> Edit this page<!--]--><!----></a></div><!----></div><div class="contributors" aria-label="Contributors" data-v-7bfa3324><span class="contributors-label" data-v-7bfa3324>Contributors: </span><span class="contributors-info" data-v-7bfa3324><!--[--><!--[--><span class="contributor" data-v-7bfa3324>SaturnTsen</span><!----><!--]--><!--]--></span></div><nav class="prev-next" data-v-7bfa3324><div class="pager" data-v-7bfa3324><a class="vp-link link pager-link prev" href="/notes/um-cv/um-cv-9/" data-v-7bfa3324><!--[--><span class="desc" data-v-7bfa3324>Previous page</span><span class="title" data-v-7bfa3324>9 Hardware, Software, PyTorch Modules</span><!--]--><!----></a></div><div class="pager" data-v-7bfa3324><a class="vp-link link pager-link next" href="/notes/um-cv/um-cv-12/" data-v-7bfa3324><!--[--><span class="desc" data-v-7bfa3324>Next page</span><span class="title" data-v-7bfa3324>12 & 13 Recurrent Neural Networks & Attention Mechanism</span><!--]--><!----></a></div></nav></footer><div id="comment" class="giscus-wrapper input-top vp-comment" vp-comment style="display:block;" data-v-380c246c><div style="display: flex;align-items: center;justify-content: center;height: 96px"><span style="--loading-icon: url(&quot;data:image/svg+xml;utf8,%3Csvg xmlns=&#39;http://www.w3.org/2000/svg&#39; preserveAspectRatio=&#39;xMidYMid&#39; viewBox=&#39;25 25 50 50&#39;%3E%3CanimateTransform attributeName=&#39;transform&#39; type=&#39;rotate&#39; dur=&#39;2s&#39; keyTimes=&#39;0;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;0;360&#39;%3E%3C/animateTransform%3E%3Ccircle cx=&#39;50&#39; cy=&#39;50&#39; r=&#39;20&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39; stroke-width=&#39;4&#39; stroke-linecap=&#39;round&#39;%3E%3Canimate attributeName=&#39;stroke-dasharray&#39; dur=&#39;1.5s&#39; keyTimes=&#39;0;0.5;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;1,200;90,200;1,200&#39;%3E%3C/animate%3E%3Canimate attributeName=&#39;stroke-dashoffset&#39; dur=&#39;1.5s&#39; keyTimes=&#39;0;0.5;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;0;-35px;-125px&#39;%3E%3C/animate%3E%3C/circle%3E%3C/svg%3E&quot;);--icon-size: 48px;display: inline-block;width: var(--icon-size);height: var(--icon-size);background-color: currentcolor;-webkit-mask-image: var(--loading-icon);mask-image: var(--loading-icon)"></span></div></div><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!--]--><button style="display:none;" type="button" class="vp-back-to-top" aria-label="back to top" data-v-a218b6db data-v-6955073a><span class="percent" data-allow-mismatch data-v-6955073a>0%</span><span class="show icon vpi-back-to-top" data-v-6955073a></span><svg aria-hidden="true" data-v-6955073a><circle cx="50%" cy="50%" data-allow-mismatch style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-6955073a></circle></svg></button><footer class="vp-footer has-sidebar" vp-footer data-v-a218b6db data-v-d9d63044><!--[--><div class="container" data-v-d9d63044><p class="message" data-v-d9d63044>Powered by <a target="_blank" href="https://v2.vuepress.vuejs.org/">VuePress</a> & <a target="_blank" href="https://theme-plume.vuejs.press">vuepress-theme-plume</a></p><p class="copyright" data-v-d9d63044>© 2024 SaturnTsen | Powered by <a href="https://vuepress.vuejs.org/" target="_blank">VuePress</a> & <a href="https://theme-plume.vuejs.press/" target="_blank">Plume</a></p></div><!--]--></footer><!--[--><!--]--><!--]--></div><!----><!--]--><!--[--><!--]--><!--]--></div><script type="module" src="/assets/app-BPmJL-vo.js" defer></script></body></html>