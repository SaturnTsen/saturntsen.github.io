<!doctype html><html lang="en-US"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.24" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.161" /><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;const isDark = um === 'dark' || (um !== 'light' && sm);document.documentElement.dataset.theme = isDark ? 'dark' : 'light';})();</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"7 & 8 CNN and its design principles","image":[""],"dateModified":"2025-02-21T17:44:17.000Z","author":[]}</script><meta property="og:url" content="https://saturntsen.github.io/notes/um-cv/um-cv-7/"><meta property="og:site_name" content="SaturnTsen"><meta property="og:title" content="7 & 8 CNN and its design principles"><meta property="og:description" content="Problem: All of the previous models flattens the input image into a vector. This loses the spatial structure of the image. CNNs are designed to work with image data directly. @C..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2025-02-21T17:44:17.000Z"><meta property="article:tag" content="computer-vision"><meta property="article:tag" content="notes"><meta property="article:modified_time" content="2025-02-21T17:44:17.000Z"><link rel="icon" href="/favicon.ico"><title>7 & 8 CNN and its design principles | SaturnTsen</title><meta name="description" content="Problem: All of the previous models flattens the input image into a vector. This loses the spatial structure of the image. CNNs are designed to work with image data directly. @C..."><link rel="preload" href="/assets/style-CU0EtsvZ.css" as="style"><link rel="stylesheet" href="/assets/style-CU0EtsvZ.css"><link rel="modulepreload" href="/assets/app-BPmJL-vo.js"><link rel="modulepreload" href="/assets/index.html-Cnmy6xJA.js"><link rel="prefetch" href="/assets/index.html-BmG4TjtM.js" as="script"><link rel="prefetch" href="/assets/index.html-BQ1NaJuK.js" as="script"><link rel="prefetch" href="/assets/index.html-vVZNyw0-.js" as="script"><link rel="prefetch" href="/assets/index.html-ZDreb-Cr.js" as="script"><link rel="prefetch" href="/assets/index.html-DnwmR40J.js" as="script"><link rel="prefetch" href="/assets/index.html-DqKRuwzh.js" as="script"><link rel="prefetch" href="/assets/index.html-BnO6pfWi.js" as="script"><link rel="prefetch" href="/assets/index.html-DqrV2oWi.js" as="script"><link rel="prefetch" href="/assets/index.html-D0505yVi.js" as="script"><link rel="prefetch" href="/assets/index.html-D4PfOoAo.js" as="script"><link rel="prefetch" href="/assets/index.html-OZtXDBX-.js" as="script"><link rel="prefetch" href="/assets/index.html-DB8OfTW0.js" as="script"><link rel="prefetch" href="/assets/index.html-Bf7LPeAD.js" as="script"><link rel="prefetch" href="/assets/index.html-Cf43ABLN.js" as="script"><link rel="prefetch" href="/assets/index.html-Dwovho6D.js" as="script"><link rel="prefetch" href="/assets/index.html-83zXgr4a.js" as="script"><link rel="prefetch" href="/assets/index.html-BLtwLLZI.js" as="script"><link rel="prefetch" href="/assets/index.html-BUZheLiL.js" as="script"><link rel="prefetch" href="/assets/index.html-DnU5krJy.js" as="script"><link rel="prefetch" href="/assets/index.html-YZrr0wBf.js" as="script"><link rel="prefetch" href="/assets/index.html-bkdbp8wU.js" as="script"><link rel="prefetch" href="/assets/index.html-CWFawhma.js" as="script"><link rel="prefetch" href="/assets/index.html-DGAYbThF.js" as="script"><link rel="prefetch" href="/assets/index.html-C46ohglK.js" as="script"><link rel="prefetch" href="/assets/index.html-CSRYllLl.js" as="script"><link rel="prefetch" href="/assets/index.html-KI14JA6V.js" as="script"><link rel="prefetch" href="/assets/index.html-CFqE0fIB.js" as="script"><link rel="prefetch" href="/assets/index.html-Cp3T_Wdm.js" as="script"><link rel="prefetch" href="/assets/index.html-C5T_LiKK.js" as="script"><link rel="prefetch" href="/assets/index.html-RGl9jofY.js" as="script"><link rel="prefetch" href="/assets/index.html-C2o8eR02.js" as="script"><link rel="prefetch" href="/assets/index.html-C4s21r8r.js" as="script"><link rel="prefetch" href="/assets/index.html-BGMJZV3q.js" as="script"><link rel="prefetch" href="/assets/index.html-BnGAEkBd.js" as="script"><link rel="prefetch" href="/assets/index.html-f3JN0p-H.js" as="script"><link rel="prefetch" href="/assets/index.html-D3JrJcoe.js" as="script"><link rel="prefetch" href="/assets/index.html-DZjtQU4g.js" as="script"><link rel="prefetch" href="/assets/index.html-DPDml381.js" as="script"><link rel="prefetch" href="/assets/index.html-D-DIxWNy.js" as="script"><link rel="prefetch" href="/assets/index.html-M3FNVy5f.js" as="script"><link rel="prefetch" href="/assets/index.html-BxBVeWuK.js" as="script"><link rel="prefetch" href="/assets/index.html-pBhdW37w.js" as="script"><link rel="prefetch" href="/assets/index.html-CMGEEupf.js" as="script"><link rel="prefetch" href="/assets/index.html-XKTeTmUn.js" as="script"><link rel="prefetch" href="/assets/index.html-Nwfmu88j.js" as="script"><link rel="prefetch" href="/assets/index.html-DzKok2JV.js" as="script"><link rel="prefetch" href="/assets/index.html-CzQ1v4M_.js" as="script"><link rel="prefetch" href="/assets/index.html-b5bM2Mm0.js" as="script"><link rel="prefetch" href="/assets/index.html-4FD4vMR_.js" as="script"><link rel="prefetch" href="/assets/index.html-CELX4Gv6.js" as="script"><link rel="prefetch" href="/assets/index.html-DAuv_XsY.js" as="script"><link rel="prefetch" href="/assets/index.html-D0HnbJld.js" as="script"><link rel="prefetch" href="/assets/index.html-cizS7tpp.js" as="script"><link rel="prefetch" href="/assets/index.html-Db9qbygp.js" as="script"><link rel="prefetch" href="/assets/index.html-BC33mLRG.js" as="script"><link rel="prefetch" href="/assets/index.html-Bgjlk9gZ.js" as="script"><link rel="prefetch" href="/assets/index.html-DNyhKRbF.js" as="script"><link rel="prefetch" href="/assets/index.html-MHkPnQ_o.js" as="script"><link rel="prefetch" href="/assets/index.html-gtR08_vy.js" as="script"><link rel="prefetch" href="/assets/index.html-CoqucIIR.js" as="script"><link rel="prefetch" href="/assets/index.html-B6BIthZU.js" as="script"><link rel="prefetch" href="/assets/index.html-BKoKQbyt.js" as="script"><link rel="prefetch" href="/assets/index.html-Bshhoiba.js" as="script"><link rel="prefetch" href="/assets/index.html-nu7-Thlm.js" as="script"><link rel="prefetch" href="/assets/index.html-B1_Iy3K7.js" as="script"><link rel="prefetch" href="/assets/index.html-C5N22UG4.js" as="script"><link rel="prefetch" href="/assets/index.html-C2esroDP.js" as="script"><link rel="prefetch" href="/assets/index.html-BwFyN5XX.js" as="script"><link rel="prefetch" href="/assets/index.html-Cr91GHno.js" as="script"><link rel="prefetch" href="/assets/index.html-D6YLzvpj.js" as="script"><link rel="prefetch" href="/assets/index.html-Ds6znEeN.js" as="script"><link rel="prefetch" href="/assets/index.html-CAJn5R2r.js" as="script"><link rel="prefetch" href="/assets/index.html-CfYOQ07j.js" as="script"><link rel="prefetch" href="/assets/index.html-BpdD7CET.js" as="script"><link rel="prefetch" href="/assets/404.html-BRfb-qcV.js" as="script"><link rel="prefetch" href="/assets/index.html-DN0XcQYf.js" as="script"><link rel="prefetch" href="/assets/index.html-BrfGEaVq.js" as="script"><link rel="prefetch" href="/assets/index.html-DPdADZ69.js" as="script"><link rel="prefetch" href="/assets/index.html-Cxk-aAuN.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-CKV1Bsxh.js" as="script"><link rel="prefetch" href="/assets/giscus-BACEvYzX.js" as="script"><link rel="prefetch" href="/assets/searchBox-default-BgWbHU8C.js" as="script"><link rel="prefetch" href="/assets/SearchBox-D0_iY6j8.js" as="script"></head><body><div id="app"><!--[--><!--[--><div class="theme-plume vp-layout" vp-container data-v-a218b6db><!--[--><!--[--><!--]--><!--[--><span tabindex="-1" data-v-7a21891b></span><a href="#VPContent" class="vp-skip-link visually-hidden" data-v-7a21891b> Skip to content </a><!--]--><!----><header class="vp-nav" data-v-a218b6db data-v-cfd3cabe><div class="vp-navbar" vp-navbar data-v-cfd3cabe data-v-f0d7e165><div class="wrapper" data-v-f0d7e165><div class="container" data-v-f0d7e165><div class="title" data-v-f0d7e165><div class="vp-navbar-title has-sidebar" data-v-f0d7e165 data-v-78317e40><a class="vp-link link no-icon title" href="/" data-v-78317e40><!--[--><!--[--><!--]--><!--[--><!--[--><!--[--><img class="vp-image dark logo" style="" src="/avatars/avatar.png" alt data-v-17210522><!--]--><!--[--><img class="vp-image light logo" style="" src="/avatars/avatar.png" alt data-v-17210522><!--]--><!--]--><!--]--><span data-v-78317e40>SaturnTsen</span><!--[--><!--]--><!--]--><!----></a></div></div><div class="content" data-v-f0d7e165><div class="content-body" data-v-f0d7e165><!--[--><!--]--><div class="vp-navbar-search search" data-v-f0d7e165><div class="search-wrapper" data-v-895d8b33><!----><div id="local-search" data-v-895d8b33><button type="button" class="mini-search mini-search-button" aria-label="Search" data-v-895d8b33><span class="mini-search-button-container"><span class="mini-search-search-icon vpi-mini-search" aria-label="search icon"></span><span class="mini-search-button-placeholder">Search</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><!--[--><!--]--><nav aria-labelledby="main-nav-aria-label" class="vp-navbar-menu menu" data-v-f0d7e165 data-v-dd33332d><span id="main-nav-aria-label" class="visually-hidden" data-v-dd33332d>Main Navigation</span><!--[--><!--[--><a class="vp-link link navbar-menu-link" href="/" tabindex="0" data-v-dd33332d data-v-caac0954><!--[--><!----><span data-v-caac0954>Home</span><!----><!--]--><!----></a><!--]--><!--[--><div class="vp-flyout vp-navbar-menu-group" data-v-dd33332d data-v-3dde86b7><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-3dde86b7><span class="text" data-v-3dde86b7><!----><!----><span data-v-3dde86b7>Notes</span><!----><span class="vpi-chevron-down text-icon" data-v-3dde86b7></span></span></button><div class="menu" data-v-3dde86b7><div class="vp-menu" data-v-3dde86b7 data-v-55c02a15><div class="items" data-v-55c02a15><!--[--><!--[--><div class="vp-menu-link" data-v-55c02a15 data-v-b7034f0e><a class="vp-link link" href="/notes/um-cv/" data-v-b7034f0e><!--[--><!----> CV-NNDL <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-55c02a15 data-v-b7034f0e><a class="vp-link link" href="/notes/leetcode/" data-v-b7034f0e><!--[--><!----> LeetCode <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-55c02a15 data-v-b7034f0e><a class="vp-link link" href="/notes/misc/" data-v-b7034f0e><!--[--><!----> Misc <!----><!--]--><!----></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!--[--><!--]--><!----><div class="vp-navbar-appearance appearance" data-v-f0d7e165 data-v-be794682><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-be794682 data-v-42684fe4 data-v-196dff46><span class="check" data-v-196dff46><span class="icon" data-v-196dff46><!--[--><span class="vpi-sun sun" data-v-42684fe4></span><span class="vpi-moon moon" data-v-42684fe4></span><!--]--></span></span></button></div><div class="vp-social-links vp-navbar-social-links social-links" data-v-f0d7e165 data-v-a8dae519 data-v-865113bf><!--[--><a class="vp-social-link no-icon" href="https://github.com/saturntsen" aria-label="github" target="_blank" rel="noopener" data-v-865113bf data-v-08f84eea><span class="vpi-social-github" /></a><!--]--></div><div class="vp-flyout vp-navbar-extra extra" data-v-f0d7e165 data-v-59aeafe8 data-v-3dde86b7><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-3dde86b7><span class="vpi-more-horizontal icon" data-v-3dde86b7></span></button><div class="menu" data-v-3dde86b7><div class="vp-menu" data-v-3dde86b7 data-v-55c02a15><!----><!--[--><!--[--><!----><div class="group" data-v-59aeafe8><div class="item appearance" data-v-59aeafe8><p class="label" data-v-59aeafe8>Appearance</p><div class="appearance-action" data-v-59aeafe8><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-59aeafe8 data-v-42684fe4 data-v-196dff46><span class="check" data-v-196dff46><span class="icon" data-v-196dff46><!--[--><span class="vpi-sun sun" data-v-42684fe4></span><span class="vpi-moon moon" data-v-42684fe4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-59aeafe8><div class="item social-links" data-v-59aeafe8><div class="vp-social-links social-links-list" data-v-59aeafe8 data-v-865113bf><!--[--><a class="vp-social-link no-icon" href="https://github.com/saturntsen" aria-label="github" target="_blank" rel="noopener" data-v-865113bf data-v-08f84eea><span class="vpi-social-github" /></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="vp-navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-f0d7e165 data-v-c65a6891><span class="container" data-v-c65a6891><span class="top" data-v-c65a6891></span><span class="middle" data-v-c65a6891></span><span class="bottom" data-v-c65a6891></span></span></button></div></div></div></div><div class="divider" data-v-f0d7e165><div class="divider-line" data-v-f0d7e165></div></div></div><!----></header><div class="vp-local-nav reached-top" data-v-a218b6db data-v-e2935d30><button class="menu" aria-expanded="false" aria-controls="SidebarNav" data-v-e2935d30><span class="vpi-align-left menu-icon" data-v-e2935d30></span><span class="menu-text" data-v-e2935d30>Menu</span></button><div class="vp-local-nav-outline-dropdown" style="--vp-vh:0px;" data-v-e2935d30 data-v-ff10071c><button data-v-ff10071c>Return to top</button><!----></div></div><aside class="vp-sidebar" vp-sidebar data-v-a218b6db data-v-4f040617><div class="curtain" data-v-4f040617></div><nav id="SidebarNav" class="nav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-4f040617><span id="sidebar-aria-label" class="visually-hidden" data-v-4f040617> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-80522f8e><section class="vp-sidebar-item sidebar-item level-0 has-active" data-v-80522f8e data-v-467b1cf7><div class="item" role="button" tabindex="0" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><h2 class="text" data-v-467b1cf7><span data-v-467b1cf7>UM-CV</span><!----></h2><!----></div><div data-v-467b1cf7 data-v-467b1cf7><div class="items" data-v-467b1cf7><!--[--><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>README</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-1/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>1 Intro</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-2/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>2 Image Classification</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-3-4/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>3 & 4 Linear Classifiers and Optimization</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-5/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>5 & 6 Neural Networks and Back Propagation</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/backprop-calc-1/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>Two-layer MLP backprop</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/backprop-calc-2/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>Batch Normalization backprop</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-7/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>7 & 8 CNN and its design principles</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-9/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>9 Hardware, Software, PyTorch Modules</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-10/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>10 & 11 Training Neural Networks</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-12/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>12 & 13 Recurrent Neural Networks & Attention Mechanism</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-14/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>14 Visualizing and understanding CNNs</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-15/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>15 Object Detection</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-16/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>16 Object Semantic Segmentation</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-17/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>17 3D Vision</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-18/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>18 Videos</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-19/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>19 Autoregressive Models, Variational Autoencoders</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-20/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>20 Generative Adversarial Networks</span><!----></p><!--]--><!----></a><!----></div><!----></div><div class="vp-sidebar-item sidebar-item level-1 is-link" data-v-467b1cf7 data-v-467b1cf7><div class="item" data-v-467b1cf7><div class="indicator" data-v-467b1cf7></div><!----><a class="vp-link link link" href="/notes/um-cv/um-cv-21/" data-v-467b1cf7><!--[--><p class="text" data-v-467b1cf7><span data-v-467b1cf7>21 Reinforcement Learning</span><!----></p><!--]--><!----></a><!----></div><!----></div><!--]--></div></div></section></div><!--]--><!--[--><!--]--></nav></aside><!--[--><div id="VPContent" vp-content class="vp-content has-sidebar" data-v-a218b6db data-v-6e8aa8db><div class="vp-doc-container has-sidebar has-aside" data-v-6e8aa8db data-v-380c246c><!--[--><!--]--><div class="container" data-v-380c246c><div class="aside" vp-outline data-v-380c246c><div class="aside-curtain" data-v-380c246c></div><div class="aside-container" data-v-380c246c><div class="aside-content" data-v-380c246c><div class="vp-doc-aside" data-v-380c246c data-v-36d29b94><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="vp-doc-aside-outline" role="navigation" data-v-36d29b94 data-v-924d914c><div class="content" data-v-924d914c><div class="outline-marker" data-v-924d914c></div><div id="doc-outline-aria-label" aria-level="2" class="outline-title" role="heading" data-v-924d914c><span data-v-924d914c>On this page</span><span class="vpi-print icon" data-v-924d914c></span></div><ul class="root" data-v-924d914c data-v-fc78de7f><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-36d29b94></div><!--[--><!--]--></div></div></div></div><div class="content" data-v-380c246c><div class="content-container" data-v-380c246c><!--[--><!--]--><main class="main" data-v-380c246c><nav class="vp-breadcrumb" data-v-380c246c data-v-389ad6f0><ol vocab="https://schema.org/" typeof="BreadcrumbList" data-v-389ad6f0><!--[--><li property="itemListElement" typeof="ListItem" data-v-389ad6f0><a class="vp-link link breadcrumb" href="/" property="item" typeof="WebPage" data-v-389ad6f0><!--[-->Home<!--]--><!----></a><span class="vpi-chevron-right" data-v-389ad6f0></span><meta property="name" content="Home" data-v-389ad6f0><meta property="position" content="1" data-v-389ad6f0></li><li property="itemListElement" typeof="ListItem" data-v-389ad6f0><span class="vp-link breadcrumb" property="item" typeof="WebPage" data-v-389ad6f0><!--[-->UM-CV<!--]--><!----></span><span class="vpi-chevron-right" data-v-389ad6f0></span><meta property="name" content="UM-CV" data-v-389ad6f0><meta property="position" content="2" data-v-389ad6f0></li><li property="itemListElement" typeof="ListItem" data-v-389ad6f0><a class="vp-link link breadcrumb current" href="/notes/um-cv/um-cv-7/" property="item" typeof="WebPage" data-v-389ad6f0><!--[-->7 &amp; 8 CNN and its design principles<!--]--><!----></a><!----><meta property="name" content="7 &amp; 8 CNN and its design principles" data-v-389ad6f0><meta property="position" content="3" data-v-389ad6f0></li><!--]--></ol></nav><!--[--><!--]--><!--[--><h1 class="vp-doc-title page-title" data-v-8dd3566a><!----> 7 &amp; 8 CNN and its design principles <!----></h1><div class="vp-doc-meta" data-v-8dd3566a><!--[--><!--]--><p class="reading-time" data-v-8dd3566a><span class="vpi-books icon" data-v-8dd3566a></span><span data-v-8dd3566a>About 2875 words</span><span data-v-8dd3566a>About 10 min</span></p><p data-v-8dd3566a><span class="vpi-tag icon" data-v-8dd3566a></span><!--[--><span class="vp-link tag vp-tag-9wic" data-v-8dd3566a><!--[-->notes<!--]--><!----></span><span class="vp-link tag vp-tag-akhq" data-v-8dd3566a><!--[-->computer-vision<!--]--><!----></span><!--]--></p><!--[--><!--]--><p class="create-time" data-v-8dd3566a><span class="vpi-clock icon" data-v-8dd3566a></span><span data-v-8dd3566a>2024-12-21</span></p></div><!--]--><!--[--><!--]--><div class="_notes_um-cv_um-cv-7_ external-link-icon-enabled vp-doc plume-content" vp-content data-v-380c246c><!--[--><!--]--><div data-v-380c246c><p>Problem: All of the previous models flattens the input image into a vector. This loses the spatial structure of the image. CNNs are designed to work with image data directly.</p><p>@Credits: <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/" target="_blank" rel="noopener noreferrer">EECS 498.007</a> | Video Lecture: <a href="https://www.youtube.com/watch?v=dJYGatp4SvA&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r" target="_blank" rel="noopener noreferrer">UM-CV</a></p><p>Personal work for the assignments of the course: <a href="https://github.com/SaturnTsen/EECS-498-007/" target="_blank" rel="noopener noreferrer">github repo</a>.</p><p><strong>Notice on Usage and Attribution</strong></p><p>These are personal class notes based on the University of Michigan EECS 498.008 / 598.008 course. They are intended solely for personal learning and academic discussion, with no commercial use.</p><p>For detailed information, please refer to the <strong><a href="#notice-on-usage-and-attribution">complete notice at the end of this document</a></strong></p><h2 id="components-of-cnn" tabindex="-1"><a class="header-anchor" href="#components-of-cnn"><span>Components of CNN</span></a></h2><ul><li>Convolutional Layers</li><li>Pooling Layers</li><li>Normalization Layers</li></ul><h3 id="convolution-layer" tabindex="-1"><a class="header-anchor" href="#convolution-layer"><span>Convolution Layer</span></a></h3><p>Image: 3x32x32</p><p>Filter: 3x5x5 Convolve the filter with the image to get a dot product. The filter is convolved with the image by sliding it across the image. The output is a 1x28x28 map.</p><p>We have multiple (for example 6) filters, each filter produces a different output. The output of the convolutional layer is a stack of 6 (28x28) maps.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-1.png" width="70%" alt="description"><br> Fig: General form of convolutional layer</div><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-2.png" width="70%" alt="description"><br> Fig: Stacking Convolutions</div><p>Typo: the length of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">b_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is 6.</p><h4 id="what-do-convolutional-filters-learn" tabindex="-1"><a class="header-anchor" href="#what-do-convolutional-filters-learn"><span>What do convolutional filters learn?</span></a></h4><p>MLP: A set of template matching filters. Each filter is a set of weights that are learned during training. The filter is convolved with the input image to produce an output.</p><p>Convolutional Network: A set of local image templates, like edge detectors, corner detectors, etc.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-3.png" width="70%" alt="description"><br> Fig: Convolutional Filters</div><h4 id="padding" tabindex="-1"><a class="header-anchor" href="#padding"><span>Padding</span></a></h4><p>A closer look at spatial dimensions</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-4.png" width="70%" alt="description"><br> Fig: Convolutional Filters</div><p>Input: 7x7 -&gt; Filter 3x3 -&gt; Output: 5x5 In general: Input W -&gt; Filter K -&gt; Output W-K+1</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-5.png" width="70%" alt="description"><br> Fig: Padding</div><h4 id="receptive-field" tabindex="-1"><a class="header-anchor" href="#receptive-field"><span>Receptive Field</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-6.png" width="70%" alt="description"><br> Fig: Receptive Field</div><p>The receptive field has two meanings: The kernel size and the field that the input dimension affects.</p><h4 id="strided-convolutions" tabindex="-1"><a class="header-anchor" href="#strided-convolutions"><span>Strided Convolutions</span></a></h4><p>Stride: The number of pixels the filter moves each time. Strided convolutions reduce the spatial dimensions of the output.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-7.png" width="70%" alt="description"><br> Fig: Strided Convolutions</div><p>Example:</p><p>Input volume 3x32x32 -&gt; 10 3x5x5 filters with stride 1, padding 2 Output volume size? Number of parameters? Number of multiply-add operations?</p><div class="language-text line-numbers-mode" data-highlighter="shiki" data-ext="text" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code class="language-text"><span class="line"><span>Answer: // [!code focus]</span></span>
<span class="line"><span>10x((32+4-5)/1+1)^2 = 10x32x32</span></span>
<span class="line"><span>10x3x5x5 + 10 = 760</span></span>
<span class="line"><span>768000, since each output pixel requires 3x5x5 multiplications. Total = 75 * (10*32*32) outputs = 768000</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Example: 1x1 convolution</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-8.png" width="70%" alt="description"><br> Fig: 1x1 Convolution</div><p>Stacking 1x1 convolution layers gives MLP operating on each input position. This preserves the spatial structure of the input.</p><h4 id="convolution-summary" tabindex="-1"><a class="header-anchor" href="#convolution-summary"><span>Convolution Summary</span></a></h4><ul><li>Input: C_in x H x W</li><li>Hyperparameters: F filters, K kernel size, S stride, P padding</li><li>Weight Matrix: C_out x C_in x K x K giving C_out filters of size KxK</li><li>Bias vector: C_out</li><li>Output: C_out x H_out x W_out</li><li>H_out = (H + 2P - K)/S + 1</li><li>W_out = (W + 2P - K)/S + 1</li></ul><h4 id="other-types-of-convolution" tabindex="-1"><a class="header-anchor" href="#other-types-of-convolution"><span>Other types of convolution</span></a></h4><p>So far: 2D convolution. There are other types of convolutions like 1D, 3D, etc.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-9.png" width="60%" alt="description"><br> Fig: 1D Convolution</div><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-10.png" width="60%" alt="description"><br> Fig: 3D Convolution</div><p>Pytorch has 1d to 3d convolutional layers.</p><h3 id="pooling-layer" tabindex="-1"><a class="header-anchor" href="#pooling-layer"><span>Pooling Layer</span></a></h3><p>The pooling layer is used to reduce the spatial dimensions of the input. It is used to reduce the number of parameters and computation in the network. It also helps in controlling overfitting.</p><p>Pooling is a form of <strong>downsampling</strong>. It reduces the spatial dimensions of the input.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-11.png" width="70%" alt="description"><br> Fig: Pooling Layer</div><p>Types of pooling:</p><ul><li>Max pooling: Takes the maximum value in the pooling window.</li><li>Average pooling: Takes the average value in the pooling window.</li></ul><p>This introduces invariance to small spatial shifts, and there is no learnable parameters in pooling.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-12.png" width="70%" alt="description"><br> Fig: Pooling Summary</div><h2 id="convolutional-networks" tabindex="-1"><a class="header-anchor" href="#convolutional-networks"><span>Convolutional Networks</span></a></h2><p>Classic architecture:</p><!----><p>Example: LeNet-5</p><table><thead><tr><th><strong>Layer</strong></th><th><strong>Output Size</strong></th><th><strong>Weight Size</strong></th></tr></thead><tbody><tr><td><strong>Input</strong></td><td>1 x 28 x 28</td><td></td></tr><tr><td><strong>Conv</strong> (C_out=20, K=5, P=2, S=1)</td><td>20 x 28 x 28</td><td>20 x 1 x 5 x 5</td></tr><tr><td><strong>ReLU</strong></td><td>20 x 28 x 28</td><td></td></tr><tr><td><strong>MaxPool</strong> (K=2, S=2)</td><td>20 x 14 x 14</td><td></td></tr><tr><td><strong>Conv</strong> (C_out=50, K=5, P=2, S=1)</td><td>50 x 14 x 14</td><td>50 x 20 x 5 x 5</td></tr><tr><td><strong>ReLU</strong></td><td>50 x 14 x 14</td><td></td></tr><tr><td><strong>MaxPool</strong> (K=2, S=2)</td><td>50 x 7 x 7</td><td></td></tr><tr><td><strong>Flatten</strong></td><td>2450</td><td></td></tr><tr><td><strong>Linear</strong> (2450-&gt;500)</td><td>50 x 7 x 7</td><td>2450 x 500</td></tr><tr><td><strong>ReLU</strong></td><td>500</td><td></td></tr><tr><td><strong>Linear</strong> (500-&gt;10)</td><td>10</td><td>500 x 10</td></tr></tbody></table><p>We tend to decrease the spatial dimensions and increase the number of channels as we go deeper into the network.</p><h3 id="training-deep-networks-batch-normalization" tabindex="-1"><a class="header-anchor" href="#training-deep-networks-batch-normalization"><span>Training Deep Networks: Batch Normalization</span></a></h3><p>Problem: Deep networks are hard to train. The gradients tend to vanish or explode as we go deeper into the network.</p><h4 id="solution-batch-normalization" tabindex="-1"><a class="header-anchor" href="#solution-batch-normalization"><span>Solution: Batch Normalization</span></a></h4><p>This helps reduce the internal covariate shift. It normalizes the activations of the network. It helps in training deeper networks.</p><p>(Joffe and Szegedy, ICML 2015, <a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener noreferrer">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>)</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-13.png" width="70%" alt="description"><br> Fig: Batch Normalization</div><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-14.png" width="70%" alt="description"><br> Fig: Calculating Batch Normalization</div><p>Calculating mean and variance over the batch means the inputs are intertangled. The estimation depends on the batch size.</p><p>During training, we use the batch mean and variance.</p><p>During testing, we use the average and variance of the entire seen data during training.</p><p>During testing, the batchnorm becomes a linear operator! We can fuse them into the previous layer.</p><h4 id="batch-normalization-for-convolutional-networks" tabindex="-1"><a class="header-anchor" href="#batch-normalization-for-convolutional-networks"><span>Batch Normalization for Convolutional Networks</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-15.png" width="70%" alt="description"><br> Fig: Batch Normalization for Convolutional Networks</div><p>See <a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener noreferrer">ICML 2015 paper</a> for more details.</p><p>Advantages and Disadvantages of Batch Normalization:</p><ul><li><p>Makes deep networks much easier to train.</p></li><li><p>Allows higher learning rates and faster convergence.</p></li><li><p>Networks become more robust to initialization.</p></li><li><p>Acts as regularization during training.</p></li><li><p>Zero overhead at test time. Can be fused into the previous layer.</p></li><li><p>Works well with feed-forward networks.</p></li><li><p>Not well understood theoretically(yet)</p></li><li><p>0 Mean is forced, and may not be ideal for all models.</p></li><li><p>Behaves differently during training and testing: this is a very common source of bugs!</p></li></ul><h4 id="instance-normalization" tabindex="-1"><a class="header-anchor" href="#instance-normalization"><span>Instance Normalization</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-16.png" width="70%" alt="description"><br> Fig: Instance Normalization</div><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/7-17.png" width="70%" alt="description"><br> Fig: Different Normalization Techniques</div><h2 id="cnn-architectures" tabindex="-1"><a class="header-anchor" href="#cnn-architectures"><span>CNN Architectures</span></a></h2><h3 id="imagenet-classification-challenge" tabindex="-1"><a class="header-anchor" href="#imagenet-classification-challenge"><span>ImageNet Classification Challenge</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-4.png" width="50%" alt="description"><br> Fig: ImageNet Classification Challenge</div><h3 id="alexnet-2012-winner-8-layers-60-million-parameters" tabindex="-1"><a class="header-anchor" href="#alexnet-2012-winner-8-layers-60-million-parameters"><span>AlexNet: 2012 winner. 8 layers, 60 million parameters.</span></a></h3><p>227×227 inputs, 5 Convolutional layers, Max pooling, 3 Fully connected layers, ReLU nonlinearity. Used &quot;local response normalization&quot;(not used anymore). Trained on two GTX 580 GPUs - only 3GB of memory each. Model split over two GPUs.</p><p>Fun fact: citations to the AlexNet</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-1.png" width="70%" alt="description"><br> Fig: AlexNet</div><table><thead><tr><th><strong>Layer</strong></th><th><strong>Input Size (C)</strong></th><th><strong>Input Size (H / W)</strong></th><th><strong>Filters</strong></th><th><strong>Kernel</strong></th><th><strong>Stride</strong></th><th><strong>Pad</strong></th><th><strong>Output Size (C)</strong></th><th><strong>Output Size (H / W)</strong></th><th><strong>Memory (KB)</strong></th><th><strong>Params (k)</strong></th><th><strong>FLOP (M)</strong></th></tr></thead><tbody><tr><td><strong>conv1</strong></td><td>3</td><td>227</td><td>64</td><td>11</td><td>4</td><td>2</td><td>64</td><td>56</td><td>784</td><td>23</td><td>73</td></tr><tr><td><strong>pool1</strong></td><td>64</td><td>56</td><td></td><td>3</td><td>2</td><td>0</td><td>64</td><td>27</td><td>182</td><td>0</td><td>0</td></tr><tr><td><strong>conv2</strong></td><td>64</td><td>27</td><td>192</td><td>5</td><td>1</td><td>2</td><td>192</td><td>27</td><td>547</td><td>307</td><td>224</td></tr><tr><td><strong>pool2</strong></td><td>192</td><td>27</td><td></td><td>3</td><td>2</td><td>0</td><td>192</td><td>13</td><td>127</td><td>0</td><td>0</td></tr><tr><td><strong>conv3</strong></td><td>192</td><td>13</td><td>384</td><td>3</td><td>1</td><td>1</td><td>384</td><td>13</td><td>254</td><td>664</td><td>112</td></tr><tr><td><strong>conv4</strong></td><td>384</td><td>13</td><td>256</td><td>3</td><td>1</td><td>1</td><td>256</td><td>13</td><td>169</td><td>885</td><td>145</td></tr><tr><td><strong>conv5</strong></td><td>256</td><td>13</td><td>256</td><td>3</td><td>1</td><td>1</td><td>256</td><td>13</td><td>169</td><td>590</td><td>100</td></tr><tr><td><strong>pool5</strong></td><td>256</td><td>13</td><td></td><td>3</td><td>2</td><td>0</td><td>256</td><td>6</td><td>36</td><td>0</td><td>0</td></tr><tr><td><strong>flatten</strong></td><td>256</td><td>6</td><td></td><td></td><td></td><td></td><td>9216</td><td></td><td>36</td><td>0</td><td>0</td></tr><tr><td><strong>fc6</strong></td><td>9216</td><td></td><td></td><td></td><td></td><td></td><td>4096</td><td></td><td>16</td><td>37,749</td><td>38</td></tr><tr><td><strong>fc7</strong></td><td>4096</td><td></td><td></td><td></td><td></td><td></td><td>4096</td><td></td><td>16</td><td>16,777</td><td>17</td></tr><tr><td><strong>fc8</strong></td><td>4096</td><td></td><td></td><td></td><td></td><td></td><td>1000</td><td></td><td>4</td><td>4,096</td><td>4</td></tr></tbody></table><p>conv1: Number of floating point operations (multiply+add) = (number of output elements) * (ops per output element) = (64 * 56 * 56) * (11 * 11 * 3) = 72,855,552 = 73M flops</p><p><strong>How is it designed?</strong> Trails and errors. Also a compromise between memory usage and computational efficiency.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-2.png" width="70%" alt="description"><br> Fig: AlexNet</div><h4 id="zfnet-a-bigger-alexnet" tabindex="-1"><a class="header-anchor" href="#zfnet-a-bigger-alexnet"><span>ZFNet: A Bigger AlexNet</span></a></h4><p>Similar to AlexNet, but with smaller filters and deeper layers. 7x7 filters in the first layer, 3x3 filters in the second layer. Deeper layers.</p><h3 id="vgg-the-principles-of-designing-a-good-network" tabindex="-1"><a class="header-anchor" href="#vgg-the-principles-of-designing-a-good-network"><span>VGG: The principles of designing a good network</span></a></h3><ul><li>All conv are 3x3 stride 1 pad 1</li><li>All max pool are 2x2 stride 2 <ul><li>Two convolutional layers together make a receptive field of 5x5, while having fewer parameters and less flops. We can also add ReLU after each layer to add non-linearity to the network.</li></ul></li><li>After pool, double the number of channels <ul><li>We want each convolutional layer to have the same computational cost.</li><li>Input1: Cx2Hx2W - Conv(3x3, C-&gt;C) - 4HWC Memory - 9C^2 Params - 4HWC flops</li><li>Input2: 2CxHxW - Conv(3x3, 2C-&gt;2C) - 4HWC Memory - 36C^2 Params - 4HWC flops</li></ul></li></ul><p><strong>5 stages:</strong> conv-conv-pool conv-conv-pool conv-conv-pool conv-conv-conv-(conv)-pool conv-conv-conv-(conv)-pool FC-FC-FC</p><p>VGG is much larger than AlexNet. It has 138 million parameters! VGG is 19.4x computationally more expensive than AlexNet.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-3.png" width="70%" alt="description"><br> Fig: VGG</div><p>Done in academia by one grad and one faculty member.</p><h3 id="google-lenet-focus-on-efficiency" tabindex="-1"><a class="header-anchor" href="#google-lenet-focus-on-efficiency"><span>Google LeNet: Focus on Efficiency</span></a></h3><p>Design an efficient convolutional network for mobile devices. 22 layers, 4 million parameters.</p><p><strong>Aggressive Stem</strong> downsamples the network at the beginning. 3x3 convolutions, 1x1 convolutions, and factorized convolutions. (CVPR 2015)</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-5.png" width="70%" alt="description"><br> Fig: Google LeNet</div><p><strong>Inception Module</strong> repeated throughout the network. It uses multiple filter sizes in parallel. It uses 1x1, 3x3, 5x5, and max pooling in parallel. It concatenates the outputs of these filters. Do all the kernel sizes in parallel and concatenate the outputs.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-6.png" width="70%" alt="description"><br> Fig: Inception Module</div><p><strong>Global Average Pooling</strong> at the end. Rather than flatting tensor, it takes the average of the tensor.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-7.png" width="70%" alt="description"><br> Fig: Global Average Pooling</div><p><strong>Auxiliary Classifiers</strong> at intermediate layers. Making gradient flow easier. (For VGG, the network is trained layerwise.)</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-8.png" width="70%" alt="description"><br> Fig: Auxiliary Classifiers</div><h3 id="resnet-cvpr-2016" tabindex="-1"><a class="header-anchor" href="#resnet-cvpr-2016"><span>ResNet (CVPR 2016)</span></a></h3><p>Deeper models does worse than shallow model!</p><p>Initial Guess: Deep model is overfitting. In fact, the training error of deeper networks is also higher than the shallower networks.</p><p>Hypothesis: This is an optimization problem. The deeper networks are harder to optimize, and in particular don&#39;t learn the identity functions to emulate the shallower networks.</p><p>Solution: Change the network so learning identity function is easier.</p><p><strong>Residual Block</strong>: Instead of learning <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>, learn <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">F(x) = H(x) - x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>. The network learns the residual function.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-9.png" width="70%" alt="description"><br> Fig: Residual Block</div><p>When backpropagating, the gradient is copied to the input. This makes the optimization easier.</p><p>Network is divided into stages like VGG. Each stage has a different number of filters.</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-10.png" width="70%" alt="description"><br> Fig: ResNet</div><h4 id="basic-block-and-bottleneck-block" tabindex="-1"><a class="header-anchor" href="#basic-block-and-bottleneck-block"><span>Basic Block and Bottleneck Block</span></a></h4><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-11.png" width="70%" alt="description"><br> Fig: Basic Block and Bottleneck Block</div><p>In 2015, ResNet ranked 1st in all five competitions!</p><p>MSRA @ ILSVRC &amp; COCO 2015 competitions</p><ul><li>ImageNet Classification</li><li>ImageNet Detection</li><li>ImageNet Localization</li><li>COCO Detection</li><li>COCO Segmentation</li></ul><h3 id="comparing-complexity" tabindex="-1"><a class="header-anchor" href="#comparing-complexity"><span>Comparing Complexity</span></a></h3><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-12.png" width="70%" alt="description"><br> Fig: Comparing Complexity</div><p>ImageNet 2016 winner: Model Ensembles</p><h3 id="resnext" tabindex="-1"><a class="header-anchor" href="#resnext"><span>ResNeXt</span></a></h3><p>ResNeXt: Aggregated Residual Transformations for Deep Neural Networks (CVPR 2017)</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-13.png" width="70%" alt="description"><br> Fig: ResNeXt</div><p>Annual ImageNet competition is no longer held after 2017. Now it is moved to Kaggle as a challenge.</p><h3 id="densenet" tabindex="-1"><a class="header-anchor" href="#densenet"><span>DenseNet</span></a></h3><p>Densely Connected Convolutional Networks (CVPR 2017)</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-14.png" width="70%" alt="description"><br> Fig: DenseNet</div><h3 id="mobilenets-tiny-networks" tabindex="-1"><a class="header-anchor" href="#mobilenets-tiny-networks"><span>MobileNets: Tiny Networks</span></a></h3><p>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (CVPR 2017)</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-15.png" width="70%" alt="description"><br> Fig: MobileNets</div><p>Also related:</p><p><a href="https://arxiv.org/abs/1707.01083" target="_blank" rel="noopener noreferrer">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices (CVPR 2018)</a></p><p><a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="noopener noreferrer">MobileNetV2: Inverted Residuals and Linear Bottlenecks (CVPR 2018)</a></p><p><a href="https://arxiv.org/abs/1807.11164" target="_blank" rel="noopener noreferrer">ShuffleNetV2: Practical Guidelines for Efficient CNN Architecture Design (ECCV 2018)</a></p><h3 id="neural-architecture-search-nas" tabindex="-1"><a class="header-anchor" href="#neural-architecture-search-nas"><span>Neural Architecture Search (NAS)</span></a></h3><p>a hot topic in deep learning research</p><div style="text-align:center;margin-bottom:1em;"><img src="/images/um-cv/8-16.png" width="70%" alt="description"><br> Fig: Neural Architecture Search</div><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary"><span>Summary</span></a></h2><p>Early work (AlexNet, VGG) focused on designing deeper networks.</p><p>Later work (ResNet, DenseNet) focused on designing more efficient networks. Recent work (MobileNets, ShuffleNet) focused on designing networks for mobile devices.</p><p>Early work (AlexNet -&gt; ZFNet -&gt; VGG) shows that bigger networks work better</p><p>GoogLeNet one of the first to focus on efficiency (aggressive stem, 1x1 bottleneck convolutions, global avg pool instead of FC layers)</p><p>ResNet showed us how to train extremely deep networks - limited only by GPU memory! Started to show diminishing returns as networks got bigger</p><p>After ResNet: Efficient networks became central: how can we improve the accuracy without increasing the complexity?</p><p>Lots of tiny networks aimed at mobile devices: MobileNet, ShuffleNet, etc</p><p>Neural Architecture Search promises to automate architecture design</p><h3 id="what-architecture-should-i-use" tabindex="-1"><a class="header-anchor" href="#what-architecture-should-i-use"><span>What architecture should I use?</span></a></h3><p>For most problems you should use an off-the-shelf architecture (e.g. ResNet, DenseNet, etc)</p><p>If you just care about accuracy, ResNet-50 or ResNet-101 is a good choice.</p><p>If you care about efficiency, MobileNet or ShuffleNet is a good choice.</p><h2 id="notice-on-usage-and-attribution" tabindex="-1"><a class="header-anchor" href="#notice-on-usage-and-attribution"><span><strong>Notice on Usage and Attribution</strong></span></a></h2><p>This note is based on the <strong>University of Michigan&#39;s publicly available course EECS 498.008 / 598.008</strong> and is intended <strong>solely for personal learning and academic discussion</strong>, with no commercial use.</p><ul><li><strong>Nature of the Notes:</strong> These notes include extensive references and citations from course materials to ensure clarity and completeness. However, they are presented as personal interpretations and summaries, not as substitutes for the original course content.</li><li><strong>Original Course Resources:</strong> Please refer to the <strong>official University of Michigan website</strong> for complete and accurate course materials.</li><li><strong>Third-Party Open Access Content:</strong> This note may reference Open Access (OA) papers or resources cited within the course materials. These materials are used under their original <strong>Open Access licenses</strong> (e.g., CC BY, CC BY-SA).</li><li><strong>Proper Attribution:</strong> Every referenced OA resource is appropriately cited, including the <strong>author, publication title, source link, and license type</strong>.</li><li><strong>Copyright Notice:</strong> All rights to third-party content remain with their respective authors or publishers.</li><li><strong>Content Removal:</strong> If you believe any content infringes on your copyright, please contact me, and I will <strong>promptly remove</strong> the content in question.</li></ul><p>Thanks to the <strong>University of Michigan</strong> and the contributors to the course for their openness and dedication to accessible education.</p></div><!----><!----><!----></div></main><footer class="vp-doc-footer" data-v-380c246c data-v-7bfa3324><!--[--><!--]--><div class="edit-info" data-v-7bfa3324><div class="edit-link" data-v-7bfa3324><a class="vp-link link no-icon edit-link-button" href="https://github.com/SaturnTsen/saturntsen.github.io/edit/main/docs/notes/UM-CV/UM-CV 7 CNN.md" target="_blank" rel="noreferrer" data-v-7bfa3324><!--[--><span class="vpi-square-pen edit-link-icon" aria-label="edit icon" data-v-7bfa3324></span> Edit this page<!--]--><!----></a></div><!----></div><div class="contributors" aria-label="Contributors" data-v-7bfa3324><span class="contributors-label" data-v-7bfa3324>Contributors: </span><span class="contributors-info" data-v-7bfa3324><!--[--><!--[--><span class="contributor" data-v-7bfa3324>SaturnTsen</span><!----><!--]--><!--]--></span></div><nav class="prev-next" data-v-7bfa3324><div class="pager" data-v-7bfa3324><a class="vp-link link pager-link prev" href="/notes/um-cv/backprop-calc-2/" data-v-7bfa3324><!--[--><span class="desc" data-v-7bfa3324>Previous page</span><span class="title" data-v-7bfa3324>Batch Normalization backprop</span><!--]--><!----></a></div><div class="pager" data-v-7bfa3324><a class="vp-link link pager-link next" href="/notes/um-cv/um-cv-9/" data-v-7bfa3324><!--[--><span class="desc" data-v-7bfa3324>Next page</span><span class="title" data-v-7bfa3324>9 Hardware, Software, PyTorch Modules</span><!--]--><!----></a></div></nav></footer><div id="comment" class="giscus-wrapper input-top vp-comment" vp-comment style="display:block;" data-v-380c246c><div style="display: flex;align-items: center;justify-content: center;height: 96px"><span style="--loading-icon: url(&quot;data:image/svg+xml;utf8,%3Csvg xmlns=&#39;http://www.w3.org/2000/svg&#39; preserveAspectRatio=&#39;xMidYMid&#39; viewBox=&#39;25 25 50 50&#39;%3E%3CanimateTransform attributeName=&#39;transform&#39; type=&#39;rotate&#39; dur=&#39;2s&#39; keyTimes=&#39;0;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;0;360&#39;%3E%3C/animateTransform%3E%3Ccircle cx=&#39;50&#39; cy=&#39;50&#39; r=&#39;20&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39; stroke-width=&#39;4&#39; stroke-linecap=&#39;round&#39;%3E%3Canimate attributeName=&#39;stroke-dasharray&#39; dur=&#39;1.5s&#39; keyTimes=&#39;0;0.5;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;1,200;90,200;1,200&#39;%3E%3C/animate%3E%3Canimate attributeName=&#39;stroke-dashoffset&#39; dur=&#39;1.5s&#39; keyTimes=&#39;0;0.5;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;0;-35px;-125px&#39;%3E%3C/animate%3E%3C/circle%3E%3C/svg%3E&quot;);--icon-size: 48px;display: inline-block;width: var(--icon-size);height: var(--icon-size);background-color: currentcolor;-webkit-mask-image: var(--loading-icon);mask-image: var(--loading-icon)"></span></div></div><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!--]--><button style="display:none;" type="button" class="vp-back-to-top" aria-label="back to top" data-v-a218b6db data-v-6955073a><span class="percent" data-allow-mismatch data-v-6955073a>0%</span><span class="show icon vpi-back-to-top" data-v-6955073a></span><svg aria-hidden="true" data-v-6955073a><circle cx="50%" cy="50%" data-allow-mismatch style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-6955073a></circle></svg></button><footer class="vp-footer has-sidebar" vp-footer data-v-a218b6db data-v-d9d63044><!--[--><div class="container" data-v-d9d63044><p class="message" data-v-d9d63044>Powered by <a target="_blank" href="https://v2.vuepress.vuejs.org/">VuePress</a> & <a target="_blank" href="https://theme-plume.vuejs.press">vuepress-theme-plume</a></p><p class="copyright" data-v-d9d63044>© 2024 SaturnTsen | Powered by <a href="https://vuepress.vuejs.org/" target="_blank">VuePress</a> & <a href="https://theme-plume.vuejs.press/" target="_blank">Plume</a></p></div><!--]--></footer><!--[--><!--]--><!--]--></div><!----><!--]--><!--[--><!--]--><!--]--></div><script type="module" src="/assets/app-BPmJL-vo.js" defer></script></body></html>